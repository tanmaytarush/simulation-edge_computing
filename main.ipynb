{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5505eae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simpy-based simulation of three edge computing servers (node 0, node1 and node2)\n",
    "The three nodes serve a wide area.\n",
    "\"\"\"\n",
    "\n",
    "# import the libraries\n",
    "import simpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from ENV import Environment\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b07b463c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_9956\\4160179475.py:73: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  e1 = tf.layers.dense(self.s, 100, tf.nn.relu6, kernel_initializer=w_initializer,\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_9956\\4160179475.py:77: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  e3 = tf.layers.dense(e1, 20, tf.nn.relu, kernel_initializer=w_initializer,\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_9956\\4160179475.py:79: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  self.q_eval = tf.layers.dense(e3, self.n_actions, tf.nn.softmax, kernel_initializer=w_initializer,\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_9956\\4160179475.py:84: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  t1 = tf.layers.dense(self.s_, 100, tf.nn.relu6, kernel_initializer=w_initializer,\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_9956\\4160179475.py:88: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  t3 = tf.layers.dense(t1, 20, tf.nn.relu, kernel_initializer=w_initializer,\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_9956\\4160179475.py:90: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  self.q_next = tf.layers.dense(t3, self.n_actions, tf.nn.softmax, kernel_initializer=w_initializer,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0  Steps: 99 Reward 0.018425022023790316 local 95 offload 5\n",
      "Episode: 1  Steps: 99 Reward 0.018104980764179192 local 190 offload 10\n",
      "Episode: 2  Steps: 99 Reward 0.0189482563339884 local 282 offload 18\n",
      "Episode: 3  Steps: 99 Reward 0.018313204101720826 local 380 offload 20\n",
      "Episode: 4  Steps: 99 Reward 0.017743566284946177 local 477 offload 23\n",
      "Episode: 5  Steps: 99 Reward 0.018353603007156626 local 568 offload 32\n",
      "Episode: 6  Steps: 99 Reward 0.017237740635757722 local 660 offload 40\n",
      "Episode: 7  Steps: 99 Reward 0.019631541441371143 local 753 offload 47\n",
      "Episode: 8  Steps: 99 Reward 0.0174818152949571 local 848 offload 52\n",
      "Episode: 9  Steps: 99 Reward 0.01677561697606401 local 945 offload 55\n",
      "Episode: 10  Steps: 99 Reward 0.017199000569448234 local 1039 offload 61\n",
      "Episode: 11  Steps: 99 Reward 0.018115214499963936 local 1135 offload 65\n",
      "Episode: 12  Steps: 99 Reward 0.016216338380352063 local 1235 offload 65\n",
      "Episode: 13  Steps: 99 Reward 0.01812561506803471 local 1327 offload 73\n",
      "Episode: 14  Steps: 99 Reward 0.017753899515543597 local 1422 offload 78\n",
      "Episode: 15  Steps: 99 Reward 0.017887130728711332 local 1518 offload 82\n",
      "Episode: 16  Steps: 99 Reward 0.017865532682531715 local 1615 offload 85\n",
      "Episode: 17  Steps: 99 Reward 0.016653496851348114 local 1712 offload 88\n",
      "Episode: 18  Steps: 99 Reward 0.018159195417250227 local 1805 offload 95\n",
      "Episode: 19  Steps: 99 Reward 0.017849875636531225 local 1899 offload 101\n",
      "Episode: 20  Steps: 99 Reward 0.016757796370873174 local 1997 offload 103\n",
      "Episode: 21  Steps: 99 Reward 0.01737072256265929 local 2091 offload 109\n",
      "Episode: 22  Steps: 99 Reward 0.017024137882268385 local 2186 offload 114\n",
      "Episode: 23  Steps: 99 Reward 0.01774035783843054 local 2282 offload 118\n",
      "Episode: 24  Steps: 99 Reward 0.018522119024219533 local 2373 offload 127\n",
      "Episode: 25  Steps: 99 Reward 0.017214551998139153 local 2471 offload 129\n",
      "Episode: 26  Steps: 99 Reward 0.01649035141568355 local 2565 offload 135\n",
      "Episode: 27  Steps: 99 Reward 0.017525473070283803 local 2660 offload 140\n",
      "Episode: 28  Steps: 99 Reward 0.017898452224235185 local 2756 offload 144\n",
      "Episode: 29  Steps: 99 Reward 0.018690753008165296 local 2849 offload 151\n",
      "Episode: 30  Steps: 99 Reward 0.018697658788450717 local 2941 offload 159\n",
      "Episode: 31  Steps: 99 Reward 0.018358892261334917 local 3035 offload 165\n",
      "Episode: 32  Steps: 99 Reward 0.017956949750856443 local 3128 offload 172\n",
      "Episode: 33  Steps: 99 Reward 0.018072385341330953 local 3223 offload 177\n",
      "Episode: 34  Steps: 99 Reward 0.016879937386488256 local 3321 offload 179\n",
      "Episode: 35  Steps: 99 Reward 0.016485168910457978 local 3416 offload 184\n",
      "Episode: 36  Steps: 99 Reward 0.018732730689526796 local 3511 offload 189\n",
      "Episode: 37  Steps: 99 Reward 0.017014293969097486 local 3607 offload 193\n",
      "Episode: 38  Steps: 99 Reward 0.01836333865145359 local 3704 offload 196\n",
      "Episode: 39  Steps: 99 Reward 0.017872327276902085 local 3798 offload 202\n",
      "Episode: 40  Steps: 99 Reward 0.018147030567914193 local 3894 offload 206\n",
      "Episode: 41  Steps: 99 Reward 0.01630568066700927 local 3993 offload 207\n",
      "Episode: 42  Steps: 99 Reward 0.017851807804676007 local 4087 offload 213\n",
      "Episode: 43  Steps: 99 Reward 0.02046841096615415 local 4174 offload 226\n",
      "Episode: 44  Steps: 99 Reward 0.018792562560258897 local 4268 offload 232\n",
      "Episode: 45  Steps: 99 Reward 0.018145830740174097 local 4363 offload 237\n",
      "Episode: 46  Steps: 99 Reward 0.01856413738856179 local 4459 offload 241\n",
      "Episode: 47  Steps: 99 Reward 0.017424480814531275 local 4555 offload 245\n",
      "Episode: 48  Steps: 99 Reward 0.01782361403733651 local 4651 offload 249\n",
      "Episode: 49  Steps: 99 Reward 0.017362284010800786 local 4747 offload 253\n",
      "Episode: 50  Steps: 99 Reward 0.017654637310304633 local 4842 offload 258\n",
      "Episode: 51  Steps: 99 Reward 0.017509370757941477 local 4936 offload 264\n",
      "Episode: 52  Steps: 99 Reward 0.01733683844094895 local 5029 offload 271\n",
      "Episode: 53  Steps: 99 Reward 0.018294796458680502 local 5120 offload 280\n",
      "Episode: 54  Steps: 99 Reward 0.017950830369415804 local 5213 offload 287\n",
      "Episode: 55  Steps: 99 Reward 0.017242916998121215 local 5310 offload 290\n",
      "Episode: 56  Steps: 99 Reward 0.019306795724276037 local 5401 offload 299\n",
      "Episode: 57  Steps: 99 Reward 0.01725849599155116 local 5499 offload 301\n",
      "Episode: 58  Steps: 99 Reward 0.01800642770111682 local 5593 offload 307\n",
      "Episode: 59  Steps: 99 Reward 0.0177690585508222 local 5687 offload 313\n",
      "Episode: 60  Steps: 99 Reward 0.01838631525935301 local 5781 offload 319\n",
      "Episode: 61  Steps: 99 Reward 0.017648661594661337 local 5879 offload 321\n",
      "Episode: 62  Steps: 99 Reward 0.017267806384074866 local 5973 offload 327\n",
      "Episode: 63  Steps: 99 Reward 0.01922685715327541 local 6063 offload 337\n",
      "Episode: 64  Steps: 99 Reward 0.016864648540269242 local 6160 offload 340\n",
      "Episode: 65  Steps: 99 Reward 0.01849026038176956 local 6252 offload 348\n",
      "Episode: 66  Steps: 99 Reward 0.018432485039383675 local 6345 offload 355\n",
      "Episode: 67  Steps: 99 Reward 0.01807335845950927 local 6442 offload 358\n",
      "Episode: 68  Steps: 99 Reward 0.01706720149002406 local 6539 offload 361\n",
      "Episode: 69  Steps: 99 Reward 0.018044239691351115 local 6636 offload 364\n",
      "Episode: 70  Steps: 99 Reward 0.016803399529883883 local 6731 offload 369\n",
      "Episode: 71  Steps: 99 Reward 0.01804473622486767 local 6826 offload 374\n",
      "Episode: 72  Steps: 99 Reward 0.017043292972459097 local 6924 offload 376\n",
      "Episode: 73  Steps: 99 Reward 0.019805647199197137 local 7015 offload 385\n",
      "Episode: 74  Steps: 99 Reward 0.016822982235291786 local 7112 offload 388\n",
      "Episode: 75  Steps: 99 Reward 0.017765114397662325 local 7207 offload 393\n",
      "Episode: 76  Steps: 99 Reward 0.019158860698412748 local 7300 offload 400\n",
      "Episode: 77  Steps: 99 Reward 0.018620453690265062 local 7393 offload 407\n",
      "Episode: 78  Steps: 99 Reward 0.017282555748277886 local 7490 offload 410\n",
      "Episode: 79  Steps: 99 Reward 0.019012477007811585 local 7585 offload 415\n",
      "Episode: 80  Steps: 99 Reward 0.017757905029214614 local 7678 offload 422\n",
      "Episode: 81  Steps: 99 Reward 0.018128802905598443 local 7772 offload 428\n",
      "Episode: 82  Steps: 99 Reward 0.01694665922595432 local 7869 offload 431\n",
      "Episode: 83  Steps: 99 Reward 0.02051316734476729 local 7959 offload 441\n",
      "Episode: 84  Steps: 99 Reward 0.017784038914589496 local 8054 offload 446\n",
      "Episode: 85  Steps: 99 Reward 0.021257547129891804 local 8143 offload 457\n",
      "Episode: 86  Steps: 99 Reward 0.016507712204371028 local 8241 offload 459\n",
      "Episode: 87  Steps: 99 Reward 0.018068697761600482 local 8335 offload 465\n",
      "Episode: 88  Steps: 99 Reward 0.018578780912349938 local 8429 offload 471\n",
      "Episode: 89  Steps: 99 Reward 0.01722329509002296 local 8525 offload 475\n",
      "Episode: 90  Steps: 99 Reward 0.016240220276791825 local 8621 offload 479\n",
      "Episode: 91  Steps: 99 Reward 0.018493276767540166 local 8715 offload 485\n",
      "Episode: 92  Steps: 99 Reward 0.018100976371975226 local 8808 offload 492\n",
      "Episode: 93  Steps: 99 Reward 0.017732054305398457 local 8906 offload 494\n",
      "Episode: 94  Steps: 99 Reward 0.017920829389459502 local 9004 offload 496\n",
      "Episode: 95  Steps: 99 Reward 0.01840254464511055 local 9095 offload 505\n",
      "Episode: 96  Steps: 99 Reward 0.018805812527347414 local 9188 offload 512\n",
      "Episode: 97  Steps: 99 Reward 0.01798189233735656 local 9284 offload 516\n",
      "Episode: 98  Steps: 99 Reward 0.01828220879242246 local 9378 offload 522\n",
      "Episode: 99  Steps: 99 Reward 0.01702062748720277 local 9474 offload 526\n",
      "Episode: 100  Steps: 99 Reward 0.1376739293206444 local 9479 offload 621\n",
      "Episode: 101  Steps: 99 Reward 0.2045734240305541 local 9487 offload 713\n",
      "Episode: 102  Steps: 99 Reward 0.29492534944352733 local 9490 offload 810\n",
      "Episode: 103  Steps: 99 Reward 0.3485157051520708 local 9493 offload 907\n",
      "Episode: 104  Steps: 99 Reward 0.40638808130790394 local 9500 offload 1000\n",
      "Episode: 105  Steps: 99 Reward 0.49608924453734504 local 9507 offload 1093\n",
      "Episode: 106  Steps: 99 Reward 0.43309232300044415 local 9510 offload 1190\n",
      "Episode: 107  Steps: 99 Reward 0.649637938419583 local 9512 offload 1288\n",
      "Episode: 108  Steps: 99 Reward 0.7134590240001848 local 9517 offload 1383\n",
      "Episode: 109  Steps: 99 Reward 0.7328107472589294 local 9522 offload 1478\n",
      "Episode: 110  Steps: 99 Reward 0.6417799628271936 local 9524 offload 1576\n",
      "Episode: 111  Steps: 99 Reward 1.170869320583558 local 9526 offload 1674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 112  Steps: 99 Reward 0.6750847072859243 local 9532 offload 1768\n",
      "Episode: 113  Steps: 99 Reward 0.994153724965516 local 9535 offload 1865\n",
      "Episode: 114  Steps: 99 Reward 0.991270467398921 local 9540 offload 1960\n",
      "Episode: 115  Steps: 99 Reward 1.1367989419316904 local 9545 offload 2055\n",
      "Episode: 116  Steps: 99 Reward 1.4183739273170248 local 9560 offload 2140\n",
      "Episode: 117  Steps: 99 Reward 2.5234657596300445 local 9564 offload 2236\n",
      "Episode: 118  Steps: 99 Reward 1.200770095630171 local 9571 offload 2329\n",
      "Episode: 119  Steps: 99 Reward 2.0378116159706656 local 9574 offload 2426\n",
      "Episode: 120  Steps: 99 Reward 1.3787446271209263 local 9582 offload 2518\n",
      "Episode: 121  Steps: 99 Reward 1.5146400700630014 local 9590 offload 2610\n",
      "Episode: 122  Steps: 99 Reward 1.5308163701965367 local 9593 offload 2707\n",
      "Episode: 123  Steps: 99 Reward 2.5333201255486046 local 9595 offload 2805\n",
      "Episode: 124  Steps: 99 Reward 1.9855220918794152 local 9602 offload 2898\n",
      "Episode: 125  Steps: 99 Reward 2.1239574483086807 local 9607 offload 2993\n",
      "Episode: 126  Steps: 99 Reward 1.0524432869490017 local 9611 offload 3089\n",
      "Episode: 127  Steps: 99 Reward 1.4121317022955657 local 9616 offload 3184\n",
      "Episode: 128  Steps: 99 Reward 1.4958907077854167 local 9618 offload 3282\n",
      "Episode: 129  Steps: 99 Reward 1.4079881182528107 local 9628 offload 3372\n",
      "Episode: 130  Steps: 99 Reward 0.8411740215878831 local 9635 offload 3465\n",
      "Episode: 131  Steps: 99 Reward 0.9178310145638803 local 9638 offload 3562\n",
      "Episode: 132  Steps: 99 Reward 1.1190702691549803 local 9643 offload 3657\n",
      "Episode: 133  Steps: 99 Reward 0.8322377883222901 local 9645 offload 3755\n",
      "Episode: 134  Steps: 99 Reward 1.0175870017041986 local 9653 offload 3847\n",
      "Episode: 135  Steps: 99 Reward 1.5646660420799137 local 9657 offload 3943\n",
      "Episode: 136  Steps: 99 Reward 2.113642789200084 local 9666 offload 4034\n",
      "Episode: 137  Steps: 99 Reward 1.1852408158677776 local 9669 offload 4131\n",
      "Episode: 138  Steps: 99 Reward 1.7417534291431338 local 9678 offload 4222\n",
      "Episode: 139  Steps: 99 Reward 2.000782375160542 local 9682 offload 4318\n",
      "Episode: 140  Steps: 99 Reward 1.7771986607667956 local 9690 offload 4410\n",
      "Episode: 141  Steps: 99 Reward 2.242194672325246 local 9694 offload 4506\n",
      "Episode: 142  Steps: 99 Reward 5.971698541695909 local 9700 offload 4600\n",
      "Episode: 143  Steps: 99 Reward 2.4695988885639863 local 9705 offload 4695\n",
      "Episode: 144  Steps: 99 Reward 1.8149704352976488 local 9709 offload 4791\n",
      "Episode: 145  Steps: 99 Reward 2.643440587768919 local 9710 offload 4890\n",
      "Episode: 146  Steps: 99 Reward 2.2034650633492316 local 9714 offload 4986\n",
      "Episode: 147  Steps: 99 Reward 1.3838061248918418 local 9719 offload 5081\n",
      "Episode: 148  Steps: 99 Reward 2.6739746651121536 local 9722 offload 5178\n",
      "Episode: 149  Steps: 99 Reward 4.836893852594972 local 9728 offload 5272\n",
      "Episode: 150  Steps: 99 Reward 2.132301563275346 local 9735 offload 5365\n",
      "Episode: 151  Steps: 99 Reward 1.5886643377194458 local 9742 offload 5458\n",
      "Episode: 152  Steps: 99 Reward 3.934459952013142 local 9744 offload 5556\n",
      "Episode: 153  Steps: 99 Reward 1.9814907330781504 local 9752 offload 5648\n",
      "Episode: 154  Steps: 99 Reward 2.1004651199055187 local 9757 offload 5743\n",
      "Episode: 155  Steps: 99 Reward 3.7494031964937506 local 9763 offload 5837\n",
      "Episode: 156  Steps: 99 Reward 3.5469597162424384 local 9768 offload 5932\n",
      "Episode: 157  Steps: 99 Reward 2.0512310421921662 local 9771 offload 6029\n",
      "Episode: 158  Steps: 99 Reward 3.1055523443201527 local 9778 offload 6122\n",
      "Episode: 159  Steps: 99 Reward 2.3037409312029373 local 9780 offload 6220\n",
      "Episode: 160  Steps: 99 Reward 2.2541237806191265 local 9786 offload 6314\n",
      "Episode: 161  Steps: 99 Reward 2.8968468493086976 local 9791 offload 6409\n",
      "Episode: 162  Steps: 99 Reward 2.6440457515715012 local 9792 offload 6508\n",
      "Episode: 163  Steps: 99 Reward 2.3601918074209105 local 9794 offload 6606\n",
      "Episode: 164  Steps: 99 Reward 3.2829982874040535 local 9800 offload 6700\n",
      "Episode: 165  Steps: 99 Reward 3.133907773845469 local 9806 offload 6794\n",
      "Episode: 166  Steps: 99 Reward 3.5657449392271507 local 9813 offload 6887\n",
      "Episode: 167  Steps: 99 Reward 2.8234221131275103 local 9821 offload 6979\n",
      "Episode: 168  Steps: 99 Reward 2.5517554827781166 local 9826 offload 7074\n",
      "Episode: 169  Steps: 99 Reward 1.8930558698060225 local 9831 offload 7169\n",
      "Episode: 170  Steps: 99 Reward 5.273468868238983 local 9837 offload 7263\n",
      "Episode: 171  Steps: 99 Reward 1.4899189491982179 local 9845 offload 7355\n",
      "Episode: 172  Steps: 99 Reward 1.4208752278463372 local 9850 offload 7450\n",
      "Episode: 173  Steps: 99 Reward 2.348657425111891 local 9852 offload 7548\n",
      "Episode: 174  Steps: 99 Reward 1.520072594519099 local 9856 offload 7644\n",
      "Episode: 175  Steps: 99 Reward 1.6226247655814405 local 9865 offload 7735\n",
      "Episode: 176  Steps: 99 Reward 1.7489935481157277 local 9872 offload 7828\n",
      "Episode: 177  Steps: 99 Reward 1.606833778919826 local 9877 offload 7923\n",
      "Episode: 178  Steps: 99 Reward 2.0519390342459145 local 9882 offload 8018\n",
      "Episode: 179  Steps: 99 Reward 1.9004214665743788 local 9886 offload 8114\n",
      "Episode: 180  Steps: 99 Reward 1.289799088019139 local 9893 offload 8207\n",
      "Episode: 181  Steps: 99 Reward 2.287242774563735 local 9897 offload 8303\n",
      "Episode: 182  Steps: 99 Reward 4.474662515298246 local 9906 offload 8394\n",
      "Episode: 183  Steps: 99 Reward 1.9294632772361278 local 9914 offload 8486\n",
      "Episode: 184  Steps: 99 Reward 2.1712559405196443 local 9920 offload 8580\n",
      "Episode: 185  Steps: 99 Reward 2.8712075826578856 local 9922 offload 8678\n",
      "Episode: 186  Steps: 99 Reward 2.197651955432042 local 9928 offload 8772\n",
      "Episode: 187  Steps: 99 Reward 2.546087743690354 local 9935 offload 8865\n",
      "Episode: 188  Steps: 99 Reward 2.5554434871021003 local 9943 offload 8957\n",
      "Episode: 189  Steps: 99 Reward 2.5734419712263885 local 9945 offload 9055\n",
      "Episode: 190  Steps: 99 Reward 4.529151800166408 local 9954 offload 9146\n",
      "Episode: 191  Steps: 99 Reward 1.7143722369491892 local 9961 offload 9239\n",
      "Episode: 192  Steps: 99 Reward 3.5241899488319017 local 9962 offload 9338\n",
      "Episode: 193  Steps: 99 Reward 2.277506528869335 local 9970 offload 9430\n",
      "Episode: 194  Steps: 99 Reward 3.581128878953516 local 9976 offload 9524\n",
      "Episode: 195  Steps: 99 Reward 2.5575877000000005 local 9976 offload 9624\n",
      "Episode: 196  Steps: 99 Reward 2.9582359216565455 local 9979 offload 9721\n",
      "Episode: 197  Steps: 99 Reward 2.873393237800359 local 9981 offload 9819\n",
      "Episode: 198  Steps: 99 Reward 2.89329079842897 local 9984 offload 9916\n",
      "Episode: 199  Steps: 99 Reward 5.4466343149787715 local 9989 offload 10011\n",
      "Episode: 200  Steps: 99 Reward 3.540792298580837 local 9994 offload 10106\n",
      "Episode: 201  Steps: 99 Reward 4.079257744118831 local 10002 offload 10198\n",
      "Episode: 202  Steps: 99 Reward 2.105663003766623 local 10010 offload 10290\n",
      "Episode: 203  Steps: 99 Reward 3.278761200025497 local 10013 offload 10387\n",
      "Episode: 204  Steps: 99 Reward 3.228589470963921 local 10019 offload 10481\n",
      "Episode: 205  Steps: 99 Reward 3.4238449313395605 local 10024 offload 10576\n",
      "Episode: 206  Steps: 99 Reward 4.441632314913307 local 10028 offload 10672\n",
      "Episode: 207  Steps: 99 Reward 4.552379899948757 local 10034 offload 10766\n",
      "Episode: 208  Steps: 99 Reward 2.552551297829867 local 10037 offload 10863\n",
      "Episode: 209  Steps: 99 Reward 2.13922528885891 local 10041 offload 10959\n",
      "Episode: 210  Steps: 99 Reward 2.0212748536290848 local 10045 offload 11055\n",
      "Episode: 211  Steps: 99 Reward 3.4989796566906772 local 10047 offload 11153\n",
      "Episode: 212  Steps: 99 Reward 4.5750705853578895 local 10053 offload 11247\n",
      "Episode: 213  Steps: 99 Reward 2.4512134517041853 local 10057 offload 11343\n",
      "Episode: 214  Steps: 99 Reward 2.0935727496867087 local 10059 offload 11441\n",
      "Episode: 215  Steps: 99 Reward 2.3947100863210453 local 10064 offload 11536\n",
      "Episode: 216  Steps: 99 Reward 2.5325203827235465 local 10070 offload 11630\n",
      "Episode: 217  Steps: 99 Reward 2.497180259786009 local 10077 offload 11723\n",
      "Episode: 218  Steps: 99 Reward 2.5047693250104586 local 10081 offload 11819\n",
      "Episode: 219  Steps: 99 Reward 4.68232359225938 local 10086 offload 11914\n",
      "Episode: 220  Steps: 99 Reward 3.6971628394708445 local 10090 offload 12010\n",
      "Episode: 221  Steps: 99 Reward 2.2498513402012623 local 10095 offload 12105\n",
      "Episode: 222  Steps: 99 Reward 2.8356026081868917 local 10100 offload 12200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 223  Steps: 99 Reward 3.362309292617779 local 10103 offload 12297\n",
      "Episode: 224  Steps: 99 Reward 3.1826498863200072 local 10106 offload 12394\n",
      "Episode: 225  Steps: 99 Reward 2.7645216628327516 local 10110 offload 12490\n",
      "Episode: 226  Steps: 99 Reward 5.947419858721073 local 10116 offload 12584\n",
      "Episode: 227  Steps: 99 Reward 3.772645949708472 local 10124 offload 12676\n",
      "Episode: 228  Steps: 99 Reward 2.2975395615284673 local 10133 offload 12767\n",
      "Episode: 229  Steps: 99 Reward 1.9924197063606124 local 10137 offload 12863\n",
      "Episode: 230  Steps: 99 Reward 1.9805683721374487 local 10142 offload 12958\n",
      "Episode: 231  Steps: 99 Reward 1.9415398962271322 local 10146 offload 13054\n",
      "Episode: 232  Steps: 99 Reward 2.559183344575583 local 10153 offload 13147\n",
      "Episode: 233  Steps: 99 Reward 3.5614836450120992 local 10156 offload 13244\n",
      "Episode: 234  Steps: 99 Reward 2.790281766150932 local 10161 offload 13339\n",
      "Episode: 235  Steps: 99 Reward 3.0027878531549828 local 10165 offload 13435\n",
      "Episode: 236  Steps: 99 Reward 3.366863460522527 local 10171 offload 13529\n",
      "Episode: 237  Steps: 99 Reward 2.5216564005013264 local 10173 offload 13627\n",
      "Episode: 238  Steps: 99 Reward 2.064147766425434 local 10176 offload 13724\n",
      "Episode: 239  Steps: 99 Reward 1.8068205216508482 local 10182 offload 13818\n",
      "Episode: 240  Steps: 99 Reward 2.932546916446425 local 10183 offload 13917\n",
      "Episode: 241  Steps: 99 Reward 2.677873483717425 local 10189 offload 14011\n",
      "Episode: 242  Steps: 99 Reward 2.868985839408336 local 10193 offload 14107\n",
      "Episode: 243  Steps: 99 Reward 3.642138687916711 local 10197 offload 14203\n",
      "Episode: 244  Steps: 99 Reward 2.361000133005511 local 10200 offload 14300\n",
      "Episode: 245  Steps: 99 Reward 2.9766107112719715 local 10204 offload 14396\n",
      "Episode: 246  Steps: 99 Reward 1.9628455034572554 local 10211 offload 14489\n",
      "Episode: 247  Steps: 99 Reward 2.2453599344940263 local 10217 offload 14583\n",
      "Episode: 248  Steps: 99 Reward 2.3277554511231116 local 10223 offload 14677\n",
      "Episode: 249  Steps: 99 Reward 2.00652171624862 local 10227 offload 14773\n",
      "Episode: 250  Steps: 99 Reward 2.0625067566606723 local 10236 offload 14864\n",
      "Episode: 251  Steps: 99 Reward 1.5900826292409442 local 10244 offload 14956\n",
      "Episode: 252  Steps: 99 Reward 2.004686624986797 local 10247 offload 15053\n",
      "Episode: 253  Steps: 99 Reward 1.619117410565394 local 10256 offload 15144\n",
      "Episode: 254  Steps: 99 Reward 1.7777658481995906 local 10262 offload 15238\n",
      "Episode: 255  Steps: 99 Reward 2.112477923831443 local 10267 offload 15333\n",
      "Episode: 256  Steps: 99 Reward 2.9653315702654686 local 10272 offload 15428\n",
      "Episode: 257  Steps: 99 Reward 2.238101791581329 local 10277 offload 15523\n",
      "Episode: 258  Steps: 99 Reward 1.9840812330224937 local 10283 offload 15617\n",
      "Episode: 259  Steps: 99 Reward 2.8546021344681147 local 10290 offload 15710\n",
      "Episode: 260  Steps: 99 Reward 1.5173940405756008 local 10296 offload 15804\n",
      "Episode: 261  Steps: 99 Reward 1.6749414056386018 local 10300 offload 15900\n",
      "Episode: 262  Steps: 99 Reward 1.5590588320877918 local 10304 offload 15996\n",
      "Episode: 263  Steps: 99 Reward 1.2666352824728715 local 10312 offload 16088\n",
      "Episode: 264  Steps: 99 Reward 1.6727433978996902 local 10316 offload 16184\n",
      "Episode: 265  Steps: 99 Reward 1.297406203292538 local 10320 offload 16280\n",
      "Episode: 266  Steps: 99 Reward 1.5281327131339522 local 10324 offload 16376\n",
      "Episode: 267  Steps: 99 Reward 1.0710622208635592 local 10326 offload 16474\n",
      "Episode: 268  Steps: 99 Reward 1.0994072691006294 local 10333 offload 16567\n",
      "Episode: 269  Steps: 99 Reward 0.8091568149256015 local 10337 offload 16663\n",
      "Episode: 270  Steps: 99 Reward 0.6820810448743317 local 10339 offload 16761\n",
      "Episode: 271  Steps: 99 Reward 0.8916765682028391 local 10343 offload 16857\n",
      "Episode: 272  Steps: 99 Reward 1.0812338262750891 local 10349 offload 16951\n",
      "Episode: 273  Steps: 99 Reward 1.109708283998848 local 10352 offload 17048\n",
      "Episode: 274  Steps: 99 Reward 0.6416451525086008 local 10359 offload 17141\n",
      "Episode: 275  Steps: 99 Reward 0.6333154091925661 local 10364 offload 17236\n",
      "Episode: 276  Steps: 99 Reward 0.5823345500127384 local 10369 offload 17331\n",
      "Episode: 277  Steps: 99 Reward 0.4169965325101465 local 10376 offload 17424\n",
      "Episode: 278  Steps: 99 Reward 0.46084773379800587 local 10383 offload 17517\n",
      "Episode: 279  Steps: 99 Reward 0.48268302258916074 local 10389 offload 17611\n",
      "Episode: 280  Steps: 99 Reward 0.18652616804783548 local 10391 offload 17709\n",
      "Episode: 281  Steps: 99 Reward 0.2982278757239552 local 10393 offload 17807\n",
      "Episode: 282  Steps: 99 Reward 0.49734530653303594 local 10399 offload 17901\n",
      "Episode: 283  Steps: 99 Reward 0.6170123398564867 local 10406 offload 17994\n",
      "Episode: 284  Steps: 99 Reward 0.46617597795923615 local 10409 offload 18091\n",
      "Episode: 285  Steps: 99 Reward 0.48035224931787385 local 10414 offload 18186\n",
      "Episode: 286  Steps: 99 Reward 0.5134292574900158 local 10418 offload 18282\n",
      "Episode: 287  Steps: 99 Reward 0.4703157383731146 local 10428 offload 18372\n",
      "Episode: 288  Steps: 99 Reward 0.7672194915827284 local 10432 offload 18468\n",
      "Episode: 289  Steps: 99 Reward 0.44175422610953574 local 10437 offload 18563\n",
      "Episode: 290  Steps: 99 Reward 0.3758068536842613 local 10445 offload 18655\n",
      "Episode: 291  Steps: 99 Reward 0.4029858165748228 local 10448 offload 18752\n",
      "Episode: 292  Steps: 99 Reward 0.5899564971992002 local 10452 offload 18848\n",
      "Episode: 293  Steps: 99 Reward 0.7272303768394518 local 10457 offload 18943\n",
      "Episode: 294  Steps: 99 Reward 0.8601018357917632 local 10459 offload 19041\n",
      "Episode: 295  Steps: 99 Reward 0.8743374708710193 local 10465 offload 19135\n",
      "Episode: 296  Steps: 99 Reward 1.6475599332371942 local 10468 offload 19232\n",
      "Episode: 297  Steps: 99 Reward 0.8161857280859288 local 10478 offload 19322\n",
      "Episode: 298  Steps: 99 Reward 2.094283275447568 local 10481 offload 19419\n",
      "Episode: 299  Steps: 99 Reward 0.9291124125122054 local 10489 offload 19511\n",
      "Episode: 300  Steps: 99 Reward 2.903789347667149 local 10493 offload 19607\n",
      "Episode: 301  Steps: 99 Reward 1.548128413296024 local 10497 offload 19703\n",
      "Episode: 302  Steps: 99 Reward 1.217153730634939 local 10501 offload 19799\n",
      "Episode: 303  Steps: 99 Reward 1.579392212424564 local 10503 offload 19897\n",
      "Episode: 304  Steps: 99 Reward 3.0439141870440043 local 10508 offload 19992\n",
      "Episode: 305  Steps: 99 Reward 0.8457475092019351 local 10512 offload 20088\n",
      "Episode: 306  Steps: 99 Reward 1.2912422912136388 local 10518 offload 20182\n",
      "Episode: 307  Steps: 99 Reward 1.7672525949417488 local 10521 offload 20279\n",
      "Episode: 308  Steps: 99 Reward 1.5880781962042076 local 10525 offload 20375\n",
      "Episode: 309  Steps: 99 Reward 0.9457149955407107 local 10530 offload 20470\n",
      "Episode: 310  Steps: 99 Reward 1.4557688630723504 local 10531 offload 20569\n",
      "Episode: 311  Steps: 99 Reward 1.7193697731902793 local 10535 offload 20665\n",
      "Episode: 312  Steps: 99 Reward 2.177576763149555 local 10538 offload 20762\n",
      "Episode: 313  Steps: 99 Reward 5.352123075045434 local 10542 offload 20858\n",
      "Episode: 314  Steps: 99 Reward 2.347434274597021 local 10546 offload 20954\n",
      "Episode: 315  Steps: 99 Reward 1.9506769441118705 local 10551 offload 21049\n",
      "Episode: 316  Steps: 99 Reward 1.7593998687953856 local 10559 offload 21141\n",
      "Episode: 317  Steps: 99 Reward 2.8985542669610953 local 10565 offload 21235\n",
      "Episode: 318  Steps: 99 Reward 2.3167949590922934 local 10566 offload 21334\n",
      "Episode: 319  Steps: 99 Reward 2.1294575556131865 local 10570 offload 21430\n",
      "Episode: 320  Steps: 99 Reward 1.5432255515781736 local 10572 offload 21528\n",
      "Episode: 321  Steps: 99 Reward 2.118349770592246 local 10573 offload 21627\n",
      "Episode: 322  Steps: 99 Reward 2.2958031295079135 local 10578 offload 21722\n",
      "Episode: 323  Steps: 99 Reward 2.1520799208827275 local 10583 offload 21817\n",
      "Episode: 324  Steps: 99 Reward 7.246478988792055 local 10586 offload 21914\n",
      "Episode: 325  Steps: 99 Reward 2.0160578612548403 local 10592 offload 22008\n",
      "Episode: 326  Steps: 99 Reward 2.0949861507176175 local 10598 offload 22102\n",
      "Episode: 327  Steps: 99 Reward 2.376283255352842 local 10602 offload 22198\n",
      "Episode: 328  Steps: 99 Reward 3.8419668187007256 local 10606 offload 22294\n",
      "Episode: 329  Steps: 99 Reward 2.542005255515452 local 10608 offload 22392\n",
      "Episode: 330  Steps: 99 Reward 2.4557820677557345 local 10611 offload 22489\n",
      "Episode: 331  Steps: 99 Reward 2.5514465961365564 local 10615 offload 22585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 332  Steps: 99 Reward 1.7464039279152075 local 10620 offload 22680\n",
      "Episode: 333  Steps: 99 Reward 2.35536354991816 local 10626 offload 22774\n",
      "Episode: 334  Steps: 99 Reward 1.4933444607653363 local 10630 offload 22870\n",
      "Episode: 335  Steps: 99 Reward 2.633904232762601 local 10633 offload 22967\n",
      "Episode: 336  Steps: 99 Reward 1.400766850623122 local 10640 offload 23060\n",
      "Episode: 337  Steps: 99 Reward 4.26739943943116 local 10647 offload 23153\n",
      "Episode: 338  Steps: 99 Reward 2.551383846793367 local 10654 offload 23246\n",
      "Episode: 339  Steps: 99 Reward 3.2805630982394534 local 10661 offload 23339\n",
      "Episode: 340  Steps: 99 Reward 2.551882807885326 local 10666 offload 23434\n",
      "Episode: 341  Steps: 99 Reward 2.1063088664875576 local 10672 offload 23528\n",
      "Episode: 342  Steps: 99 Reward 3.2768637813173624 local 10676 offload 23624\n",
      "Episode: 343  Steps: 99 Reward 2.6223640103934525 local 10680 offload 23720\n",
      "Episode: 344  Steps: 99 Reward 3.795390438084336 local 10681 offload 23819\n",
      "Episode: 345  Steps: 99 Reward 3.6421333203973023 local 10688 offload 23912\n",
      "Episode: 346  Steps: 99 Reward 2.483012244390514 local 10694 offload 24006\n",
      "Episode: 347  Steps: 99 Reward 2.9554873498382874 local 10698 offload 24102\n",
      "Episode: 348  Steps: 99 Reward 3.0293515068774646 local 10706 offload 24194\n",
      "Episode: 349  Steps: 99 Reward 3.4838492573817574 local 10708 offload 24292\n",
      "Episode: 350  Steps: 99 Reward 3.4769850652245475 local 10713 offload 24387\n",
      "Episode: 351  Steps: 99 Reward 3.264778236783799 local 10722 offload 24478\n",
      "Episode: 352  Steps: 99 Reward 3.0119919946220644 local 10726 offload 24574\n",
      "Episode: 353  Steps: 99 Reward 3.3582204801104893 local 10734 offload 24666\n",
      "Episode: 354  Steps: 99 Reward 4.893314827726969 local 10739 offload 24761\n",
      "Episode: 355  Steps: 99 Reward 3.6926550492103107 local 10742 offload 24858\n",
      "Episode: 356  Steps: 99 Reward 4.204092105586424 local 10750 offload 24950\n",
      "Episode: 357  Steps: 99 Reward 2.3027543407134115 local 10756 offload 25044\n",
      "Episode: 358  Steps: 99 Reward 7.469570736749248 local 10761 offload 25139\n",
      "Episode: 359  Steps: 99 Reward 1.9049007623969152 local 10766 offload 25234\n",
      "Episode: 360  Steps: 99 Reward 8.955703982265218 local 10771 offload 25329\n",
      "Episode: 361  Steps: 99 Reward 2.5397074010230543 local 10775 offload 25425\n",
      "Episode: 362  Steps: 99 Reward 4.082452349407039 local 10779 offload 25521\n",
      "Episode: 363  Steps: 99 Reward 3.9118040195997654 local 10783 offload 25617\n",
      "Episode: 364  Steps: 99 Reward 8.291160750442902 local 10787 offload 25713\n",
      "Episode: 365  Steps: 99 Reward 3.764050082150711 local 10791 offload 25809\n",
      "Episode: 366  Steps: 99 Reward 3.6778281799199046 local 10798 offload 25902\n",
      "Episode: 367  Steps: 99 Reward 2.4659162656071674 local 10806 offload 25994\n",
      "Episode: 368  Steps: 99 Reward 3.811708420179622 local 10811 offload 26089\n",
      "Episode: 369  Steps: 99 Reward 4.517026349296944 local 10815 offload 26185\n",
      "Episode: 370  Steps: 99 Reward 2.08317364193725 local 10820 offload 26280\n",
      "Episode: 371  Steps: 99 Reward 1.7260416773580598 local 10825 offload 26375\n",
      "Episode: 372  Steps: 99 Reward 1.6982789699017673 local 10830 offload 26470\n",
      "Episode: 373  Steps: 99 Reward 2.5098253332172518 local 10833 offload 26567\n",
      "Episode: 374  Steps: 99 Reward 2.0350099228174283 local 10837 offload 26663\n",
      "Episode: 375  Steps: 99 Reward 3.4903454139922387 local 10839 offload 26761\n",
      "Episode: 376  Steps: 99 Reward 3.8678232980907166 local 10841 offload 26859\n",
      "Episode: 377  Steps: 99 Reward 1.6477197174431308 local 10848 offload 26952\n",
      "Episode: 378  Steps: 99 Reward 2.1943182799364176 local 10854 offload 27046\n",
      "Episode: 379  Steps: 99 Reward 2.192635332339591 local 10859 offload 27141\n",
      "Episode: 380  Steps: 99 Reward 2.1606226473966537 local 10861 offload 27239\n",
      "Episode: 381  Steps: 99 Reward 1.9219843614478074 local 10869 offload 27331\n",
      "Episode: 382  Steps: 99 Reward 1.9101433058252 local 10878 offload 27422\n",
      "Episode: 383  Steps: 99 Reward 2.8635527547352586 local 10880 offload 27520\n",
      "Episode: 384  Steps: 99 Reward 2.946043862467133 local 10888 offload 27612\n",
      "Episode: 385  Steps: 99 Reward 2.2641138245891783 local 10892 offload 27708\n",
      "Episode: 386  Steps: 99 Reward 2.9813668420264112 local 10896 offload 27804\n",
      "Episode: 387  Steps: 99 Reward 2.7192814243050476 local 10903 offload 27897\n",
      "Episode: 388  Steps: 99 Reward 5.08787455325983 local 10905 offload 27995\n",
      "Episode: 389  Steps: 99 Reward 3.3041806097376547 local 10909 offload 28091\n",
      "Episode: 390  Steps: 99 Reward 6.689147012704438 local 10916 offload 28184\n",
      "Episode: 391  Steps: 99 Reward 2.2154263499999995 local 10916 offload 28284\n",
      "Episode: 392  Steps: 99 Reward 2.324722727434741 local 10922 offload 28378\n",
      "Episode: 393  Steps: 99 Reward 3.8191486971165394 local 10926 offload 28474\n",
      "Episode: 394  Steps: 99 Reward 3.5582565602610456 local 10931 offload 28569\n",
      "Episode: 395  Steps: 99 Reward 3.6664023106318306 local 10934 offload 28666\n",
      "Episode: 396  Steps: 99 Reward 4.273240855310056 local 10938 offload 28762\n",
      "Episode: 397  Steps: 99 Reward 6.566265236602646 local 10943 offload 28857\n",
      "Episode: 398  Steps: 99 Reward 2.5877482321138627 local 10950 offload 28950\n",
      "Episode: 399  Steps: 99 Reward 4.029443278101608 local 10953 offload 29047\n",
      "Episode: 400  Steps: 99 Reward 3.9105262219831354 local 10958 offload 29142\n",
      "Episode: 401  Steps: 99 Reward 2.1203614421928445 local 10963 offload 29237\n",
      "Episode: 402  Steps: 99 Reward 3.205287703414173 local 10968 offload 29332\n",
      "Episode: 403  Steps: 99 Reward 2.335101123638148 local 10969 offload 29431\n",
      "Episode: 404  Steps: 99 Reward 2.0891279453801475 local 10977 offload 29523\n",
      "Episode: 405  Steps: 99 Reward 2.056415842853797 local 10982 offload 29618\n",
      "Episode: 406  Steps: 99 Reward 2.996895953879886 local 10985 offload 29715\n",
      "Episode: 407  Steps: 99 Reward 3.5069842424030493 local 10990 offload 29810\n",
      "Episode: 408  Steps: 99 Reward 2.696812283599071 local 10995 offload 29905\n",
      "Episode: 409  Steps: 99 Reward 2.7004331788224976 local 11003 offload 29997\n",
      "Episode: 410  Steps: 99 Reward 2.3197840817603224 local 11009 offload 30091\n",
      "Episode: 411  Steps: 99 Reward 2.4837665319038176 local 11014 offload 30186\n",
      "Episode: 412  Steps: 99 Reward 2.7022333964685314 local 11020 offload 30280\n",
      "Episode: 413  Steps: 99 Reward 3.0319012934101037 local 11021 offload 30379\n",
      "Episode: 414  Steps: 99 Reward 3.912460621567091 local 11023 offload 30477\n",
      "Episode: 415  Steps: 99 Reward 2.9263147830680523 local 11027 offload 30573\n",
      "Episode: 416  Steps: 99 Reward 2.4991804781770206 local 11032 offload 30668\n",
      "Episode: 417  Steps: 99 Reward 2.5128623047321357 local 11035 offload 30765\n",
      "Episode: 418  Steps: 99 Reward 4.080734165535443 local 11038 offload 30862\n",
      "Episode: 419  Steps: 99 Reward 2.483733480067765 local 11046 offload 30954\n",
      "Episode: 420  Steps: 99 Reward 3.0227437114953775 local 11053 offload 31047\n",
      "Episode: 421  Steps: 99 Reward 4.183931830606227 local 11056 offload 31144\n",
      "Episode: 422  Steps: 99 Reward 2.769218426252628 local 11061 offload 31239\n",
      "Episode: 423  Steps: 99 Reward 3.6670852533811584 local 11065 offload 31335\n",
      "Episode: 424  Steps: 99 Reward 2.669649196595212 local 11068 offload 31432\n",
      "Episode: 425  Steps: 99 Reward 4.462784250260578 local 11073 offload 31527\n",
      "Episode: 426  Steps: 99 Reward 2.6120557368053476 local 11080 offload 31620\n",
      "Episode: 427  Steps: 99 Reward 2.832734801751206 local 11088 offload 31712\n",
      "Episode: 428  Steps: 99 Reward 8.370718847252876 local 11093 offload 31807\n",
      "Episode: 429  Steps: 99 Reward 2.641060136861012 local 11100 offload 31900\n",
      "Episode: 430  Steps: 99 Reward 2.76103276460614 local 11105 offload 31995\n",
      "Episode: 431  Steps: 99 Reward 3.376524494784221 local 11110 offload 32090\n",
      "Episode: 432  Steps: 99 Reward 4.191220777525497 local 11112 offload 32188\n",
      "Episode: 433  Steps: 99 Reward 3.5112645699755705 local 11118 offload 32282\n",
      "Episode: 434  Steps: 99 Reward 2.938724686779582 local 11122 offload 32378\n",
      "Episode: 435  Steps: 99 Reward 3.375023862821721 local 11125 offload 32475\n",
      "Episode: 436  Steps: 99 Reward 1.9002628516500653 local 11131 offload 32569\n",
      "Episode: 437  Steps: 99 Reward 5.203062576038591 local 11136 offload 32664\n",
      "Episode: 438  Steps: 99 Reward 4.823750691278956 local 11138 offload 32762\n",
      "Episode: 439  Steps: 99 Reward 4.16300614115924 local 11140 offload 32860\n",
      "Episode: 440  Steps: 99 Reward 2.2438192945458333 local 11144 offload 32956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 441  Steps: 99 Reward 1.9603302418789297 local 11148 offload 33052\n",
      "Episode: 442  Steps: 99 Reward 2.5851484851197597 local 11157 offload 33143\n",
      "Episode: 443  Steps: 99 Reward 2.854613153797317 local 11159 offload 33241\n",
      "Episode: 444  Steps: 99 Reward 4.0404204882421935 local 11165 offload 33335\n",
      "Episode: 445  Steps: 99 Reward 2.5479633242224065 local 11171 offload 33429\n",
      "Episode: 446  Steps: 99 Reward 3.909786230078055 local 11175 offload 33525\n",
      "Episode: 447  Steps: 99 Reward 2.2696425388669845 local 11182 offload 33618\n",
      "Episode: 448  Steps: 99 Reward 2.9728696577195883 local 11187 offload 33713\n",
      "Episode: 449  Steps: 99 Reward 3.227718565867613 local 11193 offload 33807\n",
      "Episode: 450  Steps: 99 Reward 2.29214654891299 local 11202 offload 33898\n",
      "Episode: 451  Steps: 99 Reward 4.535190241431631 local 11209 offload 33991\n",
      "Episode: 452  Steps: 99 Reward 3.1357081344476194 local 11212 offload 34088\n",
      "Episode: 453  Steps: 99 Reward 3.3114081603354673 local 11218 offload 34182\n",
      "Episode: 454  Steps: 99 Reward 2.9201676101092278 local 11224 offload 34276\n",
      "Episode: 455  Steps: 99 Reward 3.002329292457861 local 11227 offload 34373\n",
      "Episode: 456  Steps: 99 Reward 2.6390861113841027 local 11231 offload 34469\n",
      "Episode: 457  Steps: 99 Reward 2.376386759855451 local 11235 offload 34565\n",
      "Episode: 458  Steps: 99 Reward 2.8784358294614334 local 11239 offload 34661\n",
      "Episode: 459  Steps: 99 Reward 2.701787839062644 local 11241 offload 34759\n",
      "Episode: 460  Steps: 99 Reward 2.4083553907003328 local 11244 offload 34856\n",
      "Episode: 461  Steps: 99 Reward 2.3062815400695396 local 11254 offload 34946\n",
      "Episode: 462  Steps: 99 Reward 7.188662519139798 local 11258 offload 35042\n",
      "Episode: 463  Steps: 99 Reward 3.1319434757929936 local 11264 offload 35136\n",
      "Episode: 464  Steps: 99 Reward 3.550941097272485 local 11270 offload 35230\n",
      "Episode: 465  Steps: 99 Reward 2.7300227425542283 local 11277 offload 35323\n",
      "Episode: 466  Steps: 99 Reward 4.016338527701527 local 11285 offload 35415\n",
      "Episode: 467  Steps: 99 Reward 4.986841162994372 local 11286 offload 35514\n",
      "Episode: 468  Steps: 99 Reward 7.563199977423304 local 11292 offload 35608\n",
      "Episode: 469  Steps: 99 Reward 3.359199974939869 local 11297 offload 35703\n",
      "Episode: 470  Steps: 99 Reward 3.1575784066928634 local 11302 offload 35798\n",
      "Episode: 471  Steps: 99 Reward 2.141152340528515 local 11309 offload 35891\n",
      "Episode: 472  Steps: 99 Reward 3.417750907688526 local 11313 offload 35987\n",
      "Episode: 473  Steps: 99 Reward 3.6916802702598104 local 11319 offload 36081\n",
      "Episode: 474  Steps: 99 Reward 7.949019607444817 local 11324 offload 36176\n",
      "Episode: 475  Steps: 99 Reward 4.373123447109033 local 11331 offload 36269\n",
      "Episode: 476  Steps: 99 Reward 3.0135029606508716 local 11336 offload 36364\n",
      "Episode: 477  Steps: 99 Reward 5.795543089655122 local 11337 offload 36463\n",
      "Episode: 478  Steps: 99 Reward 4.858431748174312 local 11345 offload 36555\n",
      "Episode: 479  Steps: 99 Reward 3.1578594108483147 local 11348 offload 36652\n",
      "Episode: 480  Steps: 99 Reward 2.788045206217956 local 11353 offload 36747\n",
      "Episode: 481  Steps: 99 Reward 2.795815209565699 local 11359 offload 36841\n",
      "Episode: 482  Steps: 99 Reward 2.7862858428487995 local 11363 offload 36937\n",
      "Episode: 483  Steps: 99 Reward 3.7290923830369156 local 11365 offload 37035\n",
      "Episode: 484  Steps: 99 Reward 2.5876831837180485 local 11375 offload 37125\n",
      "Episode: 485  Steps: 99 Reward 2.9528333237024396 local 11381 offload 37219\n",
      "Episode: 486  Steps: 99 Reward 2.4892510949916042 local 11385 offload 37315\n",
      "Episode: 487  Steps: 99 Reward 2.825369476664989 local 11386 offload 37414\n",
      "Episode: 488  Steps: 99 Reward 3.489498956357138 local 11393 offload 37507\n",
      "Episode: 489  Steps: 99 Reward 4.946741533557213 local 11397 offload 37603\n",
      "Episode: 490  Steps: 99 Reward 3.203399121877902 local 11403 offload 37697\n",
      "Episode: 491  Steps: 99 Reward 3.081737883229017 local 11408 offload 37792\n",
      "Episode: 492  Steps: 99 Reward 4.20844772586756 local 11414 offload 37886\n",
      "Episode: 493  Steps: 99 Reward 1.9950611403953489 local 11421 offload 37979\n",
      "Episode: 494  Steps: 99 Reward 1.6996376393387995 local 11427 offload 38073\n",
      "Episode: 495  Steps: 99 Reward 2.2867599653539625 local 11430 offload 38170\n",
      "Episode: 496  Steps: 99 Reward 2.7277572044832947 local 11434 offload 38266\n",
      "Episode: 497  Steps: 99 Reward 1.8099194775264449 local 11435 offload 38365\n",
      "Episode: 498  Steps: 99 Reward 2.4983490631657403 local 11441 offload 38459\n",
      "Episode: 499  Steps: 99 Reward 1.882951126893782 local 11445 offload 38555\n",
      "Episode: 500  Steps: 99 Reward 1.6303315047378835 local 11454 offload 38646\n",
      "Episode: 501  Steps: 99 Reward 2.9624982713699963 local 11458 offload 38742\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 181>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    204\u001b[0m T_MEC \u001b[38;5;241m=\u001b[39m obs[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m/\u001b[39m (f_n\u001b[38;5;241m*\u001b[39mobs[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39menv\u001b[38;5;241m.\u001b[39mFmax)\n\u001b[0;32m    205\u001b[0m T_offload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m((T_trans \u001b[38;5;241m+\u001b[39m T_MEC), \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m--> 206\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mT_trans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mT_MEC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m#reward = 10\u001b[39;00m\n\u001b[0;32m    209\u001b[0m s_ \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mE:\\Workspace\\Computational Task Offloading\\ENV.py:94\u001b[0m, in \u001b[0;36mEnvironment.step\u001b[1;34m(self, actions, T_offload, T_computing)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m T_offload \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;66;03m# offloading\u001b[39;00m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;66;03m# print(T_offload, T_computing)\u001b[39;00m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;66;03m# print(\"use\", actions[1], self.residual_radio, actions[2], self.residual_computing)\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     threading\u001b[38;5;241m.\u001b[39mThread(target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_radio, args\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m     93\u001b[0m         T_offload, actions[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_radio))\u001b[38;5;241m.\u001b[39mstart()  \u001b[38;5;66;03m# target: 被執行的物件，由run()方法執行 args: target物件使用的引數\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     \u001b[43mthreading\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mThread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthread_computing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mT_computing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_computing\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m (T_offload \u001b[38;5;241m+\u001b[39m T_computing) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp \u001b[38;5;241m*\u001b[39m T_offload))\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\threading.py:897\u001b[0m, in \u001b[0;36mThread.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    895\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m _limbo[\u001b[38;5;28mself\u001b[39m]\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m--> 897\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_started\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    572\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 574\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MAX_EPISODES = 200\n",
    "MEMORY_CAPACITY = 10000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "# Deep Q Network off-policy\n",
    "class DQN(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_actions,\n",
    "            n_features,\n",
    "            learning_rate=0.1,\n",
    "            reward_decay=0.001,\n",
    "            e_greedy=0.99,\n",
    "            replace_target_iter=200,\n",
    "            memory_size=MEMORY_CAPACITY,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            # e_greedy_increment=8.684615e-05,\n",
    "            # e_greedy_increment=None,\n",
    "            output_graph=False,\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon_max = e_greedy\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        # self.epsilon_increment = e_greedy_increment\n",
    "        # self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max\n",
    "        self.epsilon = 0.9\n",
    "        # self.epsilon = 0.9\n",
    "\n",
    "        # total learning step\n",
    "        self.learn_step_counter = 0\n",
    "\n",
    "        # initialize zero memory [s, a, r, s_]\n",
    "        self.memory = np.zeros(\n",
    "            (MEMORY_CAPACITY, n_features * 2 + 2), dtype=np.float32)\n",
    "\n",
    "        # consist of [target_net, evaluate_net]\n",
    "        self._build_net()\n",
    "\n",
    "        t_params = tf.get_collection(\n",
    "            tf.GraphKeys.GLOBAL_VARIABLES, scope='target_net')\n",
    "        e_params = tf.get_collection(\n",
    "            tf.GraphKeys.GLOBAL_VARIABLES, scope='eval_net')\n",
    "\n",
    "        with tf.variable_scope('hard_replacement'):\n",
    "            self.target_replace_op = [\n",
    "                tf.assign(t, e) for t, e in zip(t_params, e_params)]\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.cost_his = []\n",
    "\n",
    "    def _build_net(self):\n",
    "        # ------------------ all inputs ------------------------\n",
    "        self.s = tf.placeholder(\n",
    "            tf.float32, [None, self.n_features], name='s')  # input State\n",
    "        self.s_ = tf.placeholder(\n",
    "            tf.float32, [None, self.n_features], name='s_')  # input Next State\n",
    "        self.r = tf.placeholder(tf.float32, [None, ], name='r')  # input Reward\n",
    "        self.a = tf.placeholder(tf.int32, [None, ], name='a')  # input Action\n",
    "\n",
    "        w_initializer, b_initializer = tf.random_normal_initializer(\n",
    "            0., 0.3), tf.constant_initializer(0.1)\n",
    "\n",
    "        # ------------------ build evaluate_net ------------------\n",
    "        with tf.variable_scope('eval_net'):\n",
    "            e1 = tf.layers.dense(self.s, 100, tf.nn.relu6, kernel_initializer=w_initializer,\n",
    "                                 bias_initializer=b_initializer, name='e1')\n",
    "            # e2 = tf.layers.dense(e1, 48, tf.nn.relu6, kernel_initializer=w_initializer,\n",
    "            #                      bias_initializer=b_initializer, name='e2')\n",
    "            e3 = tf.layers.dense(e1, 20, tf.nn.relu, kernel_initializer=w_initializer,\n",
    "                                 bias_initializer=b_initializer, name='e3')\n",
    "            self.q_eval = tf.layers.dense(e3, self.n_actions, tf.nn.softmax, kernel_initializer=w_initializer,\n",
    "                                          bias_initializer=b_initializer, name='q')\n",
    "\n",
    "        # ------------------ build target_net ------------------\n",
    "        with tf.variable_scope('target_net'):\n",
    "            t1 = tf.layers.dense(self.s_, 100, tf.nn.relu6, kernel_initializer=w_initializer,\n",
    "                                 bias_initializer=b_initializer, name='t1')\n",
    "            # t2 = tf.layers.dense(t1, 48, tf.nn.relu6, kernel_initializer=w_initializer,\n",
    "            #                      bias_initializer=b_initializer, name='t2')\n",
    "            t3 = tf.layers.dense(t1, 20, tf.nn.relu, kernel_initializer=w_initializer,\n",
    "                                 bias_initializer=b_initializer, name='t3')\n",
    "            self.q_next = tf.layers.dense(t3, self.n_actions, tf.nn.softmax, kernel_initializer=w_initializer,\n",
    "                                          bias_initializer=b_initializer, name='t4')\n",
    "\n",
    "        with tf.variable_scope('q_target'):\n",
    "            q_target = self.r + self.gamma * \\\n",
    "                tf.reduce_max(self.q_next, axis=1,\n",
    "                              name='Qmax_s_')  # shape=(None, )\n",
    "            self.q_target = tf.stop_gradient(q_target)\n",
    "        with tf.variable_scope('q_eval'):\n",
    "            a_indices = tf.stack(\n",
    "                [tf.range(tf.shape(self.a)[0], dtype=tf.int32), self.a], axis=1)\n",
    "            self.q_eval_wrt_a = tf.gather_nd(\n",
    "                params=self.q_eval, indices=a_indices)  # shape=(None, )\n",
    "        with tf.variable_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(\n",
    "                self.q_target, self.q_eval_wrt_a, name='TD_error'))\n",
    "        with tf.variable_scope('train'):\n",
    "            # self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)\n",
    "            self._train_op = tf.train.AdamOptimizer(\n",
    "                self.lr).minimize(self.loss)\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        if not hasattr(self, 'memory_counter'):\n",
    "            self.memory_counter = 0\n",
    "        transition = np.hstack((s, a, [r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # to have batch dimension when feed into tf placeholder\n",
    "        observation = observation[np.newaxis, :]\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # forward feed the observation and get q value for every actions\n",
    "            actions_value = self.sess.run(\n",
    "                self.q_eval, feed_dict={self.s: observation})\n",
    "            action = np.argmax(actions_value)\n",
    "        else:\n",
    "            action = np.random.randint(0, 2)\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        # check to replace target parameters\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self.sess.run(self.target_replace_op)\n",
    "            # print('\\ntarget_params_replaced\\n')\n",
    "\n",
    "        # sample batch memory from all memory\n",
    "        if self.memory_counter > self.memory_size:\n",
    "            sample_index = np.random.choice(\n",
    "                self.memory_size, size=self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(\n",
    "                self.memory_counter, size=self.batch_size)\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "\n",
    "        _, cost = self.sess.run(\n",
    "            [self._train_op, self.loss],\n",
    "            feed_dict={\n",
    "                self.s: batch_memory[:, :self.n_features],\n",
    "                self.a: batch_memory[:, self.n_features],\n",
    "                self.r: batch_memory[:, self.n_features + 1],\n",
    "                self.s_: batch_memory[:, -self.n_features:],\n",
    "            })\n",
    "\n",
    "        self.cost_his.append(cost)\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "\n",
    "env = Environment()\n",
    "action_space = ['0', '1']\n",
    "n_actions = len(action_space)\n",
    "n_features = 7\n",
    "RL = DQN(n_actions, n_features, output_graph=False)\n",
    "#QL = QLearningTable(actions=list(range(env.n_actions)))\n",
    "MAX_EPISODES = 3000\n",
    "T = 100\n",
    "# var = 1  # control exploration\n",
    "var = 0.1  # control exploration\n",
    "t1 = time.time()\n",
    "episode_list = []\n",
    "delay_list = []\n",
    "actions = []\n",
    "offload = 0\n",
    "local = 0\n",
    "penalty = 5\n",
    "delay = 0\n",
    "r = 0\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        time.sleep(0.1)\n",
    "    i = 0\n",
    "    for i in range(T):\n",
    "        obs = env.observed()\n",
    "        # print(\"obs\", obs[0], obs[1])\n",
    "        # Add exploration noise\n",
    "        #a0 = RL.choose_action_d(obs)\n",
    "        #print('DQN_a0', a0)\n",
    "        a = RL.choose_action(obs)\n",
    "        #print(\"DQN action\", a)\n",
    "\n",
    "        w_n = 0.2  # 0.1 or 0.99\n",
    "        f_n = 0.3  # 0.1 or 0.99\n",
    "        actions = [0.3, 0.4]\n",
    "\n",
    "        if a == 1:\n",
    "            # offload\n",
    "            R_n = w_n*obs[0]*env.Wmax * \\\n",
    "                math.log(1+(obs[5]/(w_n*obs[0]*env.Wmax*env.n0)), 2)\n",
    "            T_trans = obs[2] / R_n\n",
    "            T_MEC = obs[3] / (f_n*obs[1]*env.Fmax)\n",
    "            T_offload = round((T_trans + T_MEC), 5)\n",
    "            reward = env.step(actions, round(T_trans, 5), round(T_MEC, 5))\n",
    "\n",
    "            #reward = 10\n",
    "            s_ = obs.copy()\n",
    "            # print('S_', s_)\n",
    "            s_[0] -= (s_[0] * w_n)\n",
    "            s_[1] -= (s_[1] * f_n)\n",
    "            # print(obs, s_)\n",
    "            offload += 1\n",
    "\n",
    "        else:\n",
    "            # loacl\n",
    "            actions[0] = actions[1] = 0\n",
    "            T_local = obs[3] / obs[6]\n",
    "            E_local = env.kn * pow(obs[6], 2) * obs[3]\n",
    "            reward = env.alpha * T_local + env.beta * E_local\n",
    "\n",
    "            #reward = 10\n",
    "            # s_ = obs\n",
    "            s_ = obs.copy()\n",
    "            # s_[6] = 0\n",
    "            # print(obs, s_)\n",
    "            local += 1\n",
    "\n",
    "        #reward = reward + 0.5\n",
    "        RL.store_transition(obs, a, -reward, s_)\n",
    "\n",
    "        env.reward_list.append(reward)\n",
    "\n",
    "        if RL.memory_counter > MEMORY_CAPACITY:\n",
    "            # var = max([var * 0.9997, VAR_MIN])  # decay the action randomness\n",
    "            RL.learn()\n",
    "\n",
    "    # episode_list = np.append(episode_list, ep_reward)\n",
    "    episode_list.append(env.show_reward(T))\n",
    "    # print('reward', episode_list)\n",
    "    # print(\"actions\", actions)\n",
    "    print('Episode:', episode, ' Steps: %2d' % i, 'Reward',\n",
    "          episode_list[-1], \"local\", local, \"offload\", offload)\n",
    "    # print('OBS', obs)\n",
    "\n",
    "    # # Evaluate episode\n",
    "    # if (i + 1) % 50 == 0:\n",
    "    #     eval_policy(ddpg, env)\n",
    "\n",
    "print('Running time: ', time.time() - t1)\n",
    "plt.plot(episode_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.savefig(\"dqn.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47b10cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node1 was used; its cpu utilization was  0.73 ; task latency was  5.0\n",
      "Node1 was used; its cpu utilization was  0.99 ; task latency was  18.0\n",
      "Node0 was used; its cpu utilization was  0.98 ; task latency was  11.0\n",
      "Node2 was used; its cpu utilization was  0.98 ; task latency was  33\n",
      "Node2 was used; its cpu utilization was  0.82 ; task latency was  35\n",
      "Node0 was used; its cpu utilization was  0.82 ; task latency was  27.0\n",
      "Node1 was used; its cpu utilization was  0.91 ; task latency was  23.0\n",
      "Node2 was used; its cpu utilization was  0.83 ; task latency was  35.63\n",
      "Node1 was used; its cpu utilization was  0.89 ; task latency was  25.62\n",
      "Node0 was used; its cpu utilization was  0.82 ; task latency was  17.0\n",
      "Node2 was used; its cpu utilization was  0.84 ; task latency was  45.0\n",
      "Node2 was used; its cpu utilization was  0.99 ; task latency was  33.18\n",
      "Node0 was used; its cpu utilization was  0.99 ; task latency was  44.0\n",
      "Node1 was used; its cpu utilization was  0.91 ; task latency was  48.0\n",
      "Node0 was used; its cpu utilization was  0.94 ; task latency was  35.0\n",
      "Node2 was used; its cpu utilization was  0.94 ; task latency was  45.15\n",
      "Node1 was used; its cpu utilization was  0.81 ; task latency was  50.15\n",
      "Node1 was used; its cpu utilization was  0.91 ; task latency was  20.87\n",
      "Node2 was used; its cpu utilization was  0.89 ; task latency was  58.46\n",
      "Node2 was used; its cpu utilization was  0.75 ; task latency was  50.09\n",
      "Node2 was used; its cpu utilization was  0.76 ; task latency was  61.9\n",
      "Node0 was used; its cpu utilization was  0.81 ; task latency was  45.96\n",
      "Node0 was used; its cpu utilization was  0.84 ; task latency was  51.77\n",
      "Node1 was used; its cpu utilization was  0.78 ; task latency was  38.44\n",
      "Node1 was used; its cpu utilization was  0.94 ; task latency was  62.76\n",
      "Node0 was used; its cpu utilization was  0.94 ; task latency was  56.67\n",
      "Node1 was used; its cpu utilization was  0.98 ; task latency was  65.08\n",
      "Node2 was used; its cpu utilization was  0.97 ; task latency was  73.27\n",
      "Node2 was used; its cpu utilization was  0.94 ; task latency was  72.5\n",
      "Node1 was used; its cpu utilization was  0.9 ; task latency was  50.62\n",
      "Node2 was used; its cpu utilization was  0.9 ; task latency was  53.58\n",
      "Node0 was used; its cpu utilization was  0.85 ; task latency was  61.18\n",
      "Node1 was used; its cpu utilization was  0.85 ; task latency was  53.28\n",
      "Node1 was used; its cpu utilization was  0.87 ; task latency was  61.71\n",
      "Node1 was used; its cpu utilization was  0.79 ; task latency was  58.49\n",
      "Node2 was used; its cpu utilization was  0.88 ; task latency was  96.47\n",
      "Node0 was used; its cpu utilization was  0.74 ; task latency was  75.4\n",
      "Node2 was used; its cpu utilization was  0.85 ; task latency was  64.53\n",
      "Node0 was used; its cpu utilization was  0.87 ; task latency was  88.32\n",
      "Node2 was used; its cpu utilization was  0.99 ; task latency was  72.71\n",
      "Node1 was used; its cpu utilization was  0.92 ; task latency was  79.11\n",
      "Node0 was used; its cpu utilization was  0.89 ; task latency was  77.11\n",
      "Node2 was used; its cpu utilization was  0.89 ; task latency was  95.21\n",
      "Node2 was used; its cpu utilization was  0.98 ; task latency was  70.81\n",
      "Node0 was used; its cpu utilization was  0.94 ; task latency was  98.07\n",
      "Node2 was used; its cpu utilization was  0.9 ; task latency was  81.21\n",
      "Node2 was used; its cpu utilization was  0.97 ; task latency was  83.29\n",
      "Node1 was used; its cpu utilization was  0.97 ; task latency was  85.11\n",
      "Node1 was used; its cpu utilization was  0.85 ; task latency was  72.72\n",
      "Node0 was used; its cpu utilization was  0.93 ; task latency was  92.38\n",
      "Node2 was used; its cpu utilization was  0.84 ; task latency was  66.74\n",
      "Node1 was used; its cpu utilization was  0.9 ; task latency was  94.79\n",
      "Node0 was used; its cpu utilization was  0.9 ; task latency was  97.35\n",
      "Node1 was used; its cpu utilization was  0.98 ; task latency was  94.6\n",
      "Node2 was used; its cpu utilization was  0.9 ; task latency was  93.07\n",
      "Node0 was used; its cpu utilization was  0.84 ; task latency was  92.28\n",
      "Node2 was used; its cpu utilization was  0.84 ; task latency was  83.38\n",
      "Node1 was used; its cpu utilization was  0.84 ; task latency was  95.27\n",
      "Node2 was used; its cpu utilization was  0.83 ; task latency was  91.14\n",
      "Node1 was used; its cpu utilization was  0.95 ; task latency was  102.75\n",
      "Node2 was used; its cpu utilization was  0.93 ; task latency was  100.64\n",
      "Node0 was used; its cpu utilization was  0.78 ; task latency was  113.59\n",
      "Node0 was used; its cpu utilization was  0.96 ; task latency was  120.55\n",
      "Node1 was used; its cpu utilization was  0.85 ; task latency was  112.92\n",
      "Node2 was used; its cpu utilization was  0.85 ; task latency was  97.69\n",
      "Node2 was used; its cpu utilization was  0.77 ; task latency was  99.51\n",
      "Node1 was used; its cpu utilization was  0.99 ; task latency was  109.54\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the variables to append specific simulation data. These data will be used to plot the outputs\n",
    "total_in_append = []\n",
    "tasks_processing_append = []\n",
    "still_processing_append = []\n",
    "total_out_append = []\n",
    "tasks_in_queue_append = []\n",
    "simulation_time_append = []\n",
    "\n",
    "\n",
    "class TaskGenerator:\n",
    "    \"\"\"Generates the task\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.cpu_requested = 0\n",
    "        self.task_id = 0\n",
    "        self.process_time = 0\n",
    "        self.initiate = env.process(self.generate_task())\n",
    "\n",
    "    def generate_task(self):\n",
    "        global schedule_id\n",
    "\n",
    "        while True:\n",
    "            # inter_arrival time for tasks\n",
    "            inter_arrival = np.random.uniform(0, 6)\n",
    "\n",
    "            # append the task to the list\n",
    "            total_in_append.append(1)\n",
    "\n",
    "            # increment the task id\n",
    "            self.task_id += 1\n",
    "\n",
    "            # the cpu that is requested by the task\n",
    "            self.cpu_requested = np.random.randint(200, 500)\n",
    "\n",
    "            # the normal process time for the task\n",
    "            self.process_time = np.random.randint(5, 50)\n",
    "\n",
    "            # schedule_id is used as a routing mechanism. By changing the probability (p) values,\n",
    "            # the number of tasks to each node is changed\n",
    "            schedule_id = np.random.choice([i for i in range(3)], p=[0.33, 0.33, 0.34])\n",
    "\n",
    "            # Servicing of task in the node\n",
    "            self.env.process(ServiceTask(self.env,\n",
    "                                         self.cpu_requested,\n",
    "                                         self.process_time).service_task_in_node())\n",
    "\n",
    "            # yield next task generation\n",
    "            yield self.env.timeout(inter_arrival)\n",
    "\n",
    "            # tasks in queue\n",
    "            tasks_in_system = len(total_in_append) - len(total_out_append)\n",
    "            tasks_in_process = len(tasks_processing_append) - len(total_out_append)\n",
    "            if tasks_in_process > tasks_in_system:\n",
    "                tasks_in_queue = 0\n",
    "            else:\n",
    "                tasks_in_queue = tasks_in_system - tasks_in_process\n",
    "\n",
    "            # cpu utilization\n",
    "            cpu0_util = (cpu_node0.capacity - cpu_node0.level) / cpu_node0.capacity\n",
    "            cpu1_util = (cpu_node1.capacity - cpu_node1.level) / cpu_node1.capacity\n",
    "            cpu2_util = (cpu_node2.capacity - cpu_node2.level) / cpu_node2.capacity\n",
    "\n",
    "            # parameters to graph\n",
    "            simulation_time_append.append(self.env.now)\n",
    "            still_processing_append.append(tasks_in_process)\n",
    "            tasks_in_queue_append.append(tasks_in_queue)\n",
    "            cpu0_util_append.append(cpu0_util)\n",
    "            cpu1_util_append.append(cpu1_util)\n",
    "            cpu2_util_append.append(cpu2_util)\n",
    "\n",
    "\n",
    "class ServiceTask:\n",
    "    \"\"\"Services the task\"\"\"\n",
    "\n",
    "    def __init__(self, env, cpu_requested, process_time):\n",
    "        self.env = env\n",
    "        self.cpu_requested = cpu_requested\n",
    "        self.process_time = process_time\n",
    "\n",
    "    def service_task_in_node(self):\n",
    "        global cpu_util\n",
    "        global time_spent\n",
    "\n",
    "        # Using the for loop in this method enables the scaling up or scaling down of nodes\n",
    "        for i in range(3):\n",
    "            if schedule_id == i:\n",
    "                # register the time that the task is received\n",
    "                time_in = self.env.now\n",
    "\n",
    "                # seize the requested cpu from the ith cpu_resource list\n",
    "                yield cpu_resources[i].get(self.cpu_requested)\n",
    "\n",
    "                # compute the cpu utilization\n",
    "                cpu_in_use = cpu_resources[i].capacity - cpu_resources[i].level\n",
    "                cpu_util = cpu_in_use / cpu_node1.capacity\n",
    "\n",
    "                # seize the requested cpu for the specified process time\n",
    "                yield self.env.timeout(self.process_time)\n",
    "\n",
    "                # append the task to indicate the tasks in process\n",
    "                tasks_processing_append.append(1)\n",
    "\n",
    "                # return the cpu that was seized\n",
    "                yield cpu_resources[i].put(self.cpu_requested)\n",
    "\n",
    "                # register the time that the task is completed\n",
    "                time_out = self.env.now\n",
    "\n",
    "                # compute the time the task spends in the system\n",
    "                time_spent = round((time_out - time_in), 2)\n",
    "\n",
    "                # print some output\n",
    "                print(f\"Node{i} was used; its cpu utilization was \",\n",
    "                      round(cpu_util, 2), '; task latency was ',\n",
    "                      time_spent)\n",
    "\n",
    "                # append the task to the total out\n",
    "                total_out_append.append(1)\n",
    "\n",
    "\n",
    "# instantiate some variables\n",
    "schedule_id = 1\n",
    "cpu_util = 0\n",
    "time_spent = 0\n",
    "\n",
    "# run the simulation in real time. The higher the factor the slower the simulation model.\n",
    "sim_env = simpy.rt.RealtimeEnvironment(factor=0.01, strict=False)\n",
    "\n",
    "# set up the CPU resources and the variables for appending the utilization results\n",
    "cpu_node0 = simpy.Container(sim_env, init=1500, capacity=1500)\n",
    "cpu0_util_append = []\n",
    "cpu_node1 = simpy.Container(sim_env, init=1500, capacity=1500)\n",
    "cpu1_util_append = []\n",
    "cpu_node2 = simpy.Container(sim_env, init=1500, capacity=1500)\n",
    "cpu2_util_append = []\n",
    "\n",
    "# the cpu resources as a python list\n",
    "cpu_resources = [cpu_node0, cpu_node1, cpu_node2]\n",
    "\n",
    "# activate the simulation environment\n",
    "actuate = TaskGenerator(sim_env)\n",
    "sim_env.process(actuate.generate_task())\n",
    "\n",
    "# run the simulation  until 200 time units\n",
    "sim_env.run(until=200)\n",
    "\n",
    "# plot the charts\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, sharex=True)\n",
    "fig.suptitle('All plots')\n",
    "\n",
    "\n",
    "ax1.plot(simulation_time_append, cpu0_util_append, 'k')\n",
    "ax1.set_title('CPU0 utilization', loc='left', fontsize=6, fontweight='bold', pad=0)\n",
    "ax1.tick_params(axis='y', which='major', labelsize=5)\n",
    "ax1.set_ylim([0, 1])\n",
    "ax2.plot(simulation_time_append, cpu1_util_append, 'g')\n",
    "ax2.set_title('CPU1 utilization', loc='left', fontsize=6, fontweight='bold', pad=0)\n",
    "ax2.tick_params(axis='y', which='major', labelsize=5)\n",
    "ax2.set_ylim([0, 1])\n",
    "ax3.plot(simulation_time_append, cpu2_util_append, 'r')\n",
    "ax3.set_title('CPU2 utilization', loc='left', fontsize=6, fontweight='bold', pad=0)\n",
    "ax3.tick_params(axis='y', which='major', labelsize=5)\n",
    "ax3.set_ylim([0, 1])\n",
    "ax4.plot(simulation_time_append, tasks_in_queue_append)\n",
    "ax4.set_title('Tasks in queue', loc='left', fontsize=6, fontweight='bold', pad=0)\n",
    "ax4.tick_params(axis='y', which='major', labelsize=5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
