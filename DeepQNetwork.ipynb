{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd58bde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21404\\3418581754.py:82: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  e1 = tf.layers.dense(self.s, 100, tf.nn.relu6, kernel_initializer=w_initializer,\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21404\\3418581754.py:86: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  e3 = tf.layers.dense(e1, 20, tf.nn.relu, kernel_initializer=w_initializer,\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21404\\3418581754.py:88: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  self.q_eval = tf.layers.dense(e3, self.n_actions, tf.nn.softmax, kernel_initializer=w_initializer,\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21404\\3418581754.py:93: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  t1 = tf.layers.dense(self.s_, 100, tf.nn.relu6, kernel_initializer=w_initializer,\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21404\\3418581754.py:97: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  t3 = tf.layers.dense(t1, 20, tf.nn.relu, kernel_initializer=w_initializer,\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21404\\3418581754.py:99: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  self.q_next = tf.layers.dense(t3, self.n_actions, tf.nn.softmax, kernel_initializer=w_initializer,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0  Steps: 99 Reward 0.46355525 local 0 offload 100\n",
      "Episode: 1  Steps: 99 Reward 1.9737175850292936 local 5 offload 195\n",
      "Episode: 2  Steps: 99 Reward 8.763026332086739 local 11 offload 289\n",
      "Episode: 3  Steps: 99 Reward 6.158417261520123 local 18 offload 382\n",
      "Episode: 4  Steps: 99 Reward 4.088424435587828 local 20 offload 480\n",
      "Episode: 5  Steps: 99 Reward 3.323014116827647 local 24 offload 576\n",
      "Episode: 6  Steps: 99 Reward 2.3937714716974487 local 27 offload 673\n",
      "Episode: 7  Steps: 99 Reward 12.9063069398007 local 31 offload 769\n",
      "Episode: 8  Steps: 99 Reward 3.8032680467385545 local 35 offload 865\n",
      "Episode: 9  Steps: 99 Reward 77.58274604637234 local 38 offload 962\n",
      "Episode: 10  Steps: 99 Reward 14.64728377715127 local 43 offload 1057\n",
      "Episode: 11  Steps: 99 Reward 7.584631846356225 local 49 offload 1151\n",
      "Episode: 12  Steps: 99 Reward 40483.88335602 local 53 offload 1247\n",
      "Episode: 13  Steps: 99 Reward 39.29084542305123 local 62 offload 1338\n",
      "Episode: 14  Steps: 99 Reward 7.7866934497160845 local 67 offload 1433\n",
      "Episode: 15  Steps: 99 Reward 48.05806870464224 local 71 offload 1529\n",
      "Episode: 16  Steps: 99 Reward 13.59686857742669 local 75 offload 1625\n",
      "Episode: 17  Steps: 99 Reward 46.05927354923173 local 83 offload 1717\n",
      "Episode: 18  Steps: 99 Reward 19.21952900710851 local 90 offload 1810\n",
      "Episode: 19  Steps: 99 Reward 716.8225666391121 local 100 offload 1900\n",
      "Episode: 20  Steps: 99 Reward 60.55268515000001 local 100 offload 2000\n",
      "Episode: 21  Steps: 99 Reward 121.2600359045401 local 102 offload 2098\n",
      "Episode: 22  Steps: 99 Reward 17.08181324069388 local 105 offload 2195\n",
      "Episode: 23  Steps: 99 Reward 15.995794752993081 local 109 offload 2291\n",
      "Episode: 24  Steps: 99 Reward 126.50723962571274 local 112 offload 2388\n",
      "Episode: 25  Steps: 99 Reward 106.49307818080253 local 115 offload 2485\n",
      "Episode: 26  Steps: 99 Reward 73.66523592935823 local 123 offload 2577\n",
      "Episode: 27  Steps: 99 Reward 194.3682947306822 local 130 offload 2670\n",
      "Episode: 28  Steps: 99 Reward 19.912950645125164 local 135 offload 2765\n",
      "Episode: 29  Steps: 99 Reward 306.62744670141825 local 140 offload 2860\n",
      "Episode: 30  Steps: 99 Reward 204.9178449598313 local 145 offload 2955\n",
      "Episode: 31  Steps: 99 Reward 120.09004887025236 local 147 offload 3053\n",
      "Episode: 32  Steps: 99 Reward 76.16607264777686 local 150 offload 3150\n",
      "Episode: 33  Steps: 99 Reward 166.49418618086887 local 153 offload 3247\n",
      "Episode: 34  Steps: 99 Reward 758.607520398044 local 158 offload 3342\n",
      "Episode: 35  Steps: 99 Reward 70.40938738159592 local 160 offload 3440\n",
      "Episode: 36  Steps: 99 Reward 30.418550023805736 local 164 offload 3536\n",
      "Episode: 37  Steps: 99 Reward 298.2209101322426 local 168 offload 3632\n",
      "Episode: 38  Steps: 99 Reward 26.92354277818571 local 178 offload 3722\n",
      "Episode: 39  Steps: 99 Reward 57.59565686765673 local 181 offload 3819\n",
      "Episode: 40  Steps: 99 Reward 24.27950860276253 local 189 offload 3911\n",
      "Episode: 41  Steps: 99 Reward 31.86331329023933 local 203 offload 3997\n",
      "Episode: 42  Steps: 99 Reward 224.1844999134884 local 207 offload 4093\n",
      "Episode: 43  Steps: 99 Reward 162.05311345926737 local 210 offload 4190\n",
      "Episode: 44  Steps: 99 Reward 63.29247606647783 local 214 offload 4286\n",
      "Episode: 45  Steps: 99 Reward 306.0466146053781 local 219 offload 4381\n",
      "Episode: 46  Steps: 99 Reward 23.070529702153866 local 223 offload 4477\n",
      "Episode: 47  Steps: 99 Reward 32.06674308072926 local 231 offload 4569\n",
      "Episode: 48  Steps: 99 Reward 30.479430788159153 local 236 offload 4664\n",
      "Episode: 49  Steps: 99 Reward 76.2409695133294 local 242 offload 4758\n",
      "Episode: 50  Steps: 99 Reward 80.51375847453903 local 249 offload 4851\n",
      "Episode: 51  Steps: 99 Reward 38.87475238466159 local 260 offload 4940\n",
      "Episode: 52  Steps: 99 Reward 30.502629945443022 local 264 offload 5036\n",
      "Episode: 53  Steps: 99 Reward 104.1790378351381 local 267 offload 5133\n",
      "Episode: 54  Steps: 99 Reward 288.6436239092035 local 269 offload 5231\n",
      "Episode: 55  Steps: 99 Reward 105.10554740258249 local 271 offload 5329\n",
      "Episode: 56  Steps: 99 Reward 18.90384287155954 local 276 offload 5424\n",
      "Episode: 57  Steps: 99 Reward 3807.1242897378393 local 280 offload 5520\n",
      "Episode: 58  Steps: 99 Reward 69.78201942547844 local 283 offload 5617\n",
      "Episode: 59  Steps: 99 Reward 74.05507733710724 local 288 offload 5712\n",
      "Episode: 60  Steps: 99 Reward 20.12697890798281 local 291 offload 5809\n",
      "Episode: 61  Steps: 99 Reward 13.803263255637608 local 296 offload 5904\n",
      "Episode: 62  Steps: 99 Reward 1957.8921405841027 local 301 offload 5999\n",
      "Episode: 63  Steps: 99 Reward 116.20510613107484 local 307 offload 6093\n",
      "Episode: 64  Steps: 99 Reward 38.23138041009742 local 317 offload 6183\n",
      "Episode: 65  Steps: 99 Reward 531.1432763921623 local 322 offload 6278\n",
      "Episode: 66  Steps: 99 Reward 119.10883515433783 local 329 offload 6371\n",
      "Episode: 67  Steps: 99 Reward 41.09451940350472 local 337 offload 6463\n",
      "Episode: 68  Steps: 99 Reward 26.77018752486065 local 345 offload 6555\n",
      "Episode: 69  Steps: 99 Reward 26.841953609086577 local 350 offload 6650\n",
      "Episode: 70  Steps: 99 Reward 35.69182568735803 local 360 offload 6740\n",
      "Episode: 71  Steps: 99 Reward 38.465599648261815 local 365 offload 6835\n",
      "Episode: 72  Steps: 99 Reward 33.84149304154037 local 367 offload 6933\n",
      "Episode: 73  Steps: 99 Reward 41.297606857690965 local 371 offload 7029\n",
      "Episode: 74  Steps: 99 Reward 48.1708781382865 local 376 offload 7124\n",
      "Episode: 75  Steps: 99 Reward 139.131565198865 local 382 offload 7218\n",
      "Episode: 76  Steps: 99 Reward 61.33712921618283 local 388 offload 7312\n",
      "Episode: 77  Steps: 99 Reward 58.22213988396763 local 394 offload 7406\n",
      "Episode: 78  Steps: 99 Reward 40.93599585908 local 401 offload 7499\n",
      "Episode: 79  Steps: 99 Reward 59.64708922454571 local 407 offload 7593\n",
      "Episode: 80  Steps: 99 Reward 24.013986028668374 local 412 offload 7688\n",
      "Episode: 81  Steps: 99 Reward 92.37954069186105 local 418 offload 7782\n",
      "Episode: 82  Steps: 99 Reward 36.41790246770906 local 422 offload 7878\n",
      "Episode: 83  Steps: 99 Reward 23.848371650814887 local 428 offload 7972\n",
      "Episode: 84  Steps: 99 Reward 157.61302021364335 local 432 offload 8068\n",
      "Episode: 85  Steps: 99 Reward 38.68272841012328 local 435 offload 8165\n",
      "Episode: 86  Steps: 99 Reward 34.41855248267079 local 440 offload 8260\n",
      "Episode: 87  Steps: 99 Reward 43.80246828014915 local 445 offload 8355\n",
      "Episode: 88  Steps: 99 Reward 41.24974489771582 local 451 offload 8449\n",
      "Episode: 89  Steps: 99 Reward 27.67388990429523 local 455 offload 8545\n",
      "Episode: 90  Steps: 99 Reward 22.071552826326265 local 460 offload 8640\n",
      "Episode: 91  Steps: 99 Reward 52.838164272548774 local 467 offload 8733\n",
      "Episode: 92  Steps: 99 Reward 26.623731127857877 local 474 offload 8826\n",
      "Episode: 93  Steps: 99 Reward 38.268032657396645 local 476 offload 8924\n",
      "Episode: 94  Steps: 99 Reward 27.476655544419415 local 479 offload 9021\n",
      "Episode: 95  Steps: 99 Reward 137.9570377032029 local 483 offload 9117\n",
      "Episode: 96  Steps: 99 Reward 36.610039783858625 local 486 offload 9214\n",
      "Episode: 97  Steps: 99 Reward 21.78999568339267 local 494 offload 9306\n",
      "Episode: 98  Steps: 99 Reward 18.4827074491575 local 501 offload 9399\n",
      "Episode: 99  Steps: 99 Reward 68.40373428634523 local 505 offload 9495\n",
      "Episode: 100  Steps: 99 Reward 0.036298670385288265 local 602 offload 9498\n",
      "Episode: 101  Steps: 99 Reward 0.02601881182987985 local 700 offload 9500\n",
      "Episode: 102  Steps: 99 Reward 0.03566560148769285 local 795 offload 9505\n",
      "Episode: 103  Steps: 99 Reward 0.07705311178310971 local 887 offload 9513\n",
      "Episode: 104  Steps: 99 Reward 0.017574927819269507 local 986 offload 9514\n",
      "Episode: 105  Steps: 99 Reward 0.02873262305195529 local 1082 offload 9518\n",
      "Episode: 106  Steps: 99 Reward 0.06352800754575531 local 1173 offload 9527\n",
      "Episode: 107  Steps: 99 Reward 0.01949355342257482 local 1272 offload 9528\n",
      "Episode: 108  Steps: 99 Reward 0.07679447789626517 local 1363 offload 9537\n",
      "Episode: 109  Steps: 99 Reward 0.026526116729999404 local 1460 offload 9540\n",
      "Episode: 110  Steps: 99 Reward 0.01692153973051947 local 1558 offload 9542\n",
      "Episode: 111  Steps: 99 Reward 0.02364601215635513 local 1651 offload 9549\n",
      "Episode: 112  Steps: 99 Reward 0.02384434650185263 local 1745 offload 9555\n",
      "Episode: 113  Steps: 99 Reward 0.019208423011052318 local 1841 offload 9559\n",
      "Episode: 114  Steps: 99 Reward 0.023447011651683284 local 1934 offload 9566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 115  Steps: 99 Reward 0.018502163744459718 local 2031 offload 9569\n",
      "Episode: 116  Steps: 99 Reward 0.018001337097743722 local 2128 offload 9572\n",
      "Episode: 117  Steps: 99 Reward 0.018782679768585612 local 2222 offload 9578\n",
      "Episode: 118  Steps: 99 Reward 0.018742657435201503 local 2317 offload 9583\n",
      "Episode: 119  Steps: 99 Reward 0.018195250182811266 local 2412 offload 9588\n",
      "Episode: 120  Steps: 99 Reward 0.017480436093457636 local 2508 offload 9592\n",
      "Episode: 121  Steps: 99 Reward 0.017789505599827306 local 2603 offload 9597\n",
      "Episode: 122  Steps: 99 Reward 0.018795523938097924 local 2698 offload 9602\n",
      "Episode: 123  Steps: 99 Reward 0.01746276362503889 local 2794 offload 9606\n",
      "Episode: 124  Steps: 99 Reward 0.018319126716313056 local 2892 offload 9608\n",
      "Episode: 125  Steps: 99 Reward 0.017728050780106367 local 2988 offload 9612\n",
      "Episode: 126  Steps: 99 Reward 0.017772502717643114 local 3085 offload 9615\n",
      "Episode: 127  Steps: 99 Reward 0.017539990513032396 local 3181 offload 9619\n",
      "Episode: 128  Steps: 99 Reward 0.01829155732075754 local 3275 offload 9625\n",
      "Episode: 129  Steps: 99 Reward 0.019372082048535372 local 3366 offload 9634\n",
      "Episode: 130  Steps: 99 Reward 0.018754389135270392 local 3461 offload 9639\n",
      "Episode: 131  Steps: 99 Reward 0.01767766067022375 local 3557 offload 9643\n",
      "Episode: 132  Steps: 99 Reward 0.017246535678806324 local 3655 offload 9645\n",
      "Episode: 133  Steps: 99 Reward 0.01892876441406507 local 3749 offload 9651\n",
      "Episode: 134  Steps: 99 Reward 0.017309233603923814 local 3846 offload 9654\n",
      "Episode: 135  Steps: 99 Reward 0.01786757985598997 local 3943 offload 9657\n",
      "Episode: 136  Steps: 99 Reward 0.01702350328426775 local 4041 offload 9659\n",
      "Episode: 137  Steps: 99 Reward 0.01667217294448389 local 4140 offload 9660\n",
      "Episode: 138  Steps: 99 Reward 0.016855077716123493 local 4236 offload 9664\n",
      "Episode: 139  Steps: 99 Reward 0.017947039531141924 local 4328 offload 9672\n",
      "Episode: 140  Steps: 99 Reward 0.017381892358201055 local 4424 offload 9676\n",
      "Episode: 141  Steps: 99 Reward 0.017798015137823305 local 4523 offload 9677\n",
      "Episode: 142  Steps: 99 Reward 0.018518284647078356 local 4615 offload 9685\n",
      "Episode: 143  Steps: 99 Reward 0.01729114807799384 local 4713 offload 9687\n",
      "Episode: 144  Steps: 99 Reward 0.018828748054410257 local 4808 offload 9692\n",
      "Episode: 145  Steps: 99 Reward 0.01763691086911779 local 4905 offload 9695\n",
      "Episode: 146  Steps: 99 Reward 0.020088493696898198 local 4993 offload 9707\n",
      "Episode: 147  Steps: 99 Reward 0.018292692520070687 local 5088 offload 9712\n",
      "Episode: 148  Steps: 99 Reward 0.01724207357368804 local 5184 offload 9716\n",
      "Episode: 149  Steps: 99 Reward 0.016495460599408148 local 5281 offload 9719\n",
      "Episode: 150  Steps: 99 Reward 0.017855292492335865 local 5377 offload 9723\n",
      "Episode: 151  Steps: 99 Reward 0.018934462593473356 local 5471 offload 9729\n",
      "Episode: 152  Steps: 99 Reward 0.019508647432532163 local 5564 offload 9736\n",
      "Episode: 153  Steps: 99 Reward 0.01603643496018002 local 5660 offload 9740\n",
      "Episode: 154  Steps: 99 Reward 0.017405704226853128 local 5755 offload 9745\n",
      "Episode: 155  Steps: 99 Reward 0.018107726590024055 local 5850 offload 9750\n",
      "Episode: 156  Steps: 99 Reward 0.017213829380319515 local 5947 offload 9753\n",
      "Episode: 157  Steps: 99 Reward 0.01983619886457715 local 6037 offload 9763\n",
      "Episode: 158  Steps: 99 Reward 0.0165179857863181 local 6132 offload 9768\n",
      "Episode: 159  Steps: 99 Reward 0.01756610986199666 local 6228 offload 9772\n",
      "Episode: 160  Steps: 99 Reward 0.01898770912300894 local 6322 offload 9778\n",
      "Episode: 161  Steps: 99 Reward 0.018037298915488578 local 6419 offload 9781\n",
      "Episode: 162  Steps: 99 Reward 0.01933233704623518 local 6510 offload 9790\n",
      "Episode: 163  Steps: 99 Reward 0.01686620336319614 local 6607 offload 9793\n",
      "Episode: 164  Steps: 99 Reward 0.017222374854376582 local 6701 offload 9799\n",
      "Episode: 165  Steps: 99 Reward 0.017899187738099155 local 6795 offload 9805\n",
      "Episode: 166  Steps: 99 Reward 0.018192115653008326 local 6889 offload 9811\n",
      "Episode: 167  Steps: 99 Reward 0.016885437665641975 local 6987 offload 9813\n",
      "Episode: 168  Steps: 99 Reward 0.01722208901762525 local 7081 offload 9819\n",
      "Episode: 169  Steps: 99 Reward 0.018188179271068784 local 7176 offload 9824\n",
      "Episode: 170  Steps: 99 Reward 0.017059320282824218 local 7274 offload 9826\n",
      "Episode: 171  Steps: 99 Reward 0.01658966486242415 local 7373 offload 9827\n",
      "Episode: 172  Steps: 99 Reward 0.016752707641039123 local 7469 offload 9831\n",
      "Episode: 173  Steps: 99 Reward 0.018470125649590788 local 7561 offload 9839\n",
      "Episode: 174  Steps: 99 Reward 0.018379900316918517 local 7654 offload 9846\n",
      "Episode: 175  Steps: 99 Reward 0.017489175204722953 local 7748 offload 9852\n",
      "Episode: 176  Steps: 99 Reward 0.0183047762516571 local 7843 offload 9857\n",
      "Episode: 177  Steps: 99 Reward 0.018703953602671266 local 7935 offload 9865\n",
      "Episode: 178  Steps: 99 Reward 0.017707799072740803 local 8029 offload 9871\n",
      "Episode: 179  Steps: 99 Reward 0.01812043236002161 local 8120 offload 9880\n",
      "Episode: 180  Steps: 99 Reward 0.017713956402967995 local 8217 offload 9883\n",
      "Episode: 181  Steps: 99 Reward 0.01816247290375901 local 8311 offload 9889\n",
      "Episode: 182  Steps: 99 Reward 0.0172869500913816 local 8407 offload 9893\n",
      "Episode: 183  Steps: 99 Reward 0.016519991817005475 local 8504 offload 9896\n",
      "Episode: 184  Steps: 99 Reward 0.01795466635136843 local 8600 offload 9900\n",
      "Episode: 185  Steps: 99 Reward 0.01650619774890857 local 8696 offload 9904\n",
      "Episode: 186  Steps: 99 Reward 0.017065083512901383 local 8791 offload 9909\n",
      "Episode: 187  Steps: 99 Reward 0.01821944621360146 local 8886 offload 9914\n",
      "Episode: 188  Steps: 99 Reward 0.017632207906238456 local 8982 offload 9918\n",
      "Episode: 189  Steps: 99 Reward 0.01789366880140455 local 9079 offload 9921\n",
      "Episode: 190  Steps: 99 Reward 0.017079618754510383 local 9177 offload 9923\n",
      "Episode: 191  Steps: 99 Reward 0.017902323731820425 local 9271 offload 9929\n",
      "Episode: 192  Steps: 99 Reward 0.01708013727051879 local 9366 offload 9934\n",
      "Episode: 193  Steps: 99 Reward 0.017683859284051764 local 9460 offload 9940\n",
      "Episode: 194  Steps: 99 Reward 0.018086268863730468 local 9553 offload 9947\n",
      "Episode: 195  Steps: 99 Reward 0.01757682855727185 local 9650 offload 9950\n",
      "Episode: 196  Steps: 99 Reward 0.01708849182861601 local 9748 offload 9952\n",
      "Episode: 197  Steps: 99 Reward 0.018203743538705554 local 9842 offload 9958\n",
      "Episode: 198  Steps: 99 Reward 0.01723984020253982 local 9938 offload 9962\n",
      "Episode: 199  Steps: 99 Reward 0.017411513788948934 local 10033 offload 9967\n",
      "Episode: 200  Steps: 99 Reward 0.017790779822118918 local 10123 offload 9977\n",
      "Episode: 201  Steps: 99 Reward 0.018220137407081116 local 10213 offload 9987\n",
      "Episode: 202  Steps: 99 Reward 0.017969803257688398 local 10304 offload 9996\n",
      "Episode: 203  Steps: 99 Reward 0.01776368669653127 local 10396 offload 10004\n",
      "Episode: 204  Steps: 99 Reward 0.017370453794094718 local 10488 offload 10012\n",
      "Episode: 205  Steps: 99 Reward 0.01720219765707459 local 10585 offload 10015\n",
      "Episode: 206  Steps: 99 Reward 0.017047155571059923 local 10682 offload 10018\n",
      "Episode: 207  Steps: 99 Reward 0.01802688612978508 local 10780 offload 10020\n",
      "Episode: 208  Steps: 99 Reward 0.017668608394019084 local 10874 offload 10026\n",
      "Episode: 209  Steps: 99 Reward 0.016496211128738836 local 10969 offload 10031\n",
      "Episode: 210  Steps: 99 Reward 0.01722674744358335 local 11063 offload 10037\n",
      "Episode: 211  Steps: 99 Reward 0.016839167783914095 local 11158 offload 10042\n",
      "Episode: 212  Steps: 99 Reward 0.017259793794081935 local 11256 offload 10044\n",
      "Episode: 213  Steps: 99 Reward 0.018414453418995697 local 11352 offload 10048\n",
      "Episode: 214  Steps: 99 Reward 0.01763135279013747 local 11446 offload 10054\n",
      "Episode: 215  Steps: 99 Reward 0.018222134177272656 local 11543 offload 10057\n",
      "Episode: 216  Steps: 99 Reward 0.018582986643958382 local 11640 offload 10060\n",
      "Episode: 217  Steps: 99 Reward 0.01800156548470019 local 11740 offload 10060\n",
      "Episode: 218  Steps: 99 Reward 0.017221304865922375 local 11837 offload 10063\n",
      "Episode: 219  Steps: 99 Reward 0.017549395664233063 local 11931 offload 10069\n",
      "Episode: 220  Steps: 99 Reward 0.016900353335733618 local 12025 offload 10075\n",
      "Episode: 221  Steps: 99 Reward 0.017198541386099404 local 12119 offload 10081\n",
      "Episode: 222  Steps: 99 Reward 0.016525162224714608 local 12215 offload 10085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 223  Steps: 99 Reward 0.017404135391362496 local 12311 offload 10089\n",
      "Episode: 224  Steps: 99 Reward 0.0179108910671618 local 12404 offload 10096\n",
      "Episode: 225  Steps: 99 Reward 0.016487584508420042 local 12500 offload 10100\n",
      "Episode: 226  Steps: 99 Reward 0.01671321356518037 local 12599 offload 10101\n",
      "Episode: 227  Steps: 99 Reward 0.017552192634570484 local 12697 offload 10103\n",
      "Episode: 228  Steps: 99 Reward 0.01835752205400875 local 12791 offload 10109\n",
      "Episode: 229  Steps: 99 Reward 0.01677219235794019 local 12888 offload 10112\n",
      "Episode: 230  Steps: 99 Reward 0.017391761605966304 local 12985 offload 10115\n",
      "Episode: 231  Steps: 99 Reward 0.018255154842020074 local 13078 offload 10122\n",
      "Episode: 232  Steps: 99 Reward 0.017683942326586568 local 13176 offload 10124\n",
      "Episode: 233  Steps: 99 Reward 0.01708199253038206 local 13271 offload 10129\n",
      "Episode: 234  Steps: 99 Reward 0.016019738981907995 local 13363 offload 10137\n",
      "Episode: 235  Steps: 99 Reward 0.016986751544350413 local 13457 offload 10143\n",
      "Episode: 236  Steps: 99 Reward 0.017612667316997586 local 13555 offload 10145\n",
      "Episode: 237  Steps: 99 Reward 0.017186955811936876 local 13653 offload 10147\n",
      "Episode: 238  Steps: 99 Reward 0.017243048424039852 local 13746 offload 10154\n",
      "Episode: 239  Steps: 99 Reward 0.01726514944967765 local 13841 offload 10159\n",
      "Episode: 240  Steps: 99 Reward 0.01632093856656912 local 13934 offload 10166\n",
      "Episode: 241  Steps: 99 Reward 0.017744444872992603 local 14028 offload 10172\n",
      "Episode: 242  Steps: 99 Reward 0.015650958795137393 local 14123 offload 10177\n",
      "Episode: 243  Steps: 99 Reward 0.016949714773188608 local 14216 offload 10184\n",
      "Episode: 244  Steps: 99 Reward 0.016828427551300394 local 14311 offload 10189\n",
      "Episode: 245  Steps: 99 Reward 0.016634122642459552 local 14405 offload 10195\n",
      "Episode: 246  Steps: 99 Reward 0.01643079514553661 local 14499 offload 10201\n",
      "Episode: 247  Steps: 99 Reward 0.01683574824059853 local 14594 offload 10206\n",
      "Episode: 248  Steps: 99 Reward 0.01798480899768051 local 14689 offload 10211\n",
      "Episode: 249  Steps: 99 Reward 0.017926156730637396 local 14784 offload 10216\n",
      "Episode: 250  Steps: 99 Reward 0.01793024783976665 local 14879 offload 10221\n",
      "Episode: 251  Steps: 99 Reward 0.016256978764754074 local 14975 offload 10225\n",
      "Episode: 252  Steps: 99 Reward 0.01737996692143814 local 15072 offload 10228\n",
      "Episode: 253  Steps: 99 Reward 0.01815792188965867 local 15169 offload 10231\n",
      "Episode: 254  Steps: 99 Reward 0.016914316481026155 local 15264 offload 10236\n",
      "Episode: 255  Steps: 99 Reward 0.018174665129282914 local 15357 offload 10243\n",
      "Episode: 256  Steps: 99 Reward 0.01684151199165976 local 15452 offload 10248\n",
      "Episode: 257  Steps: 99 Reward 0.017287027433153877 local 15545 offload 10255\n",
      "Episode: 258  Steps: 99 Reward 0.018789679568626066 local 15636 offload 10264\n",
      "Episode: 259  Steps: 99 Reward 0.01697881170575218 local 15730 offload 10270\n",
      "Episode: 260  Steps: 99 Reward 0.017351325355068437 local 15821 offload 10279\n",
      "Episode: 261  Steps: 99 Reward 0.018249772620638687 local 15915 offload 10285\n",
      "Episode: 262  Steps: 99 Reward 0.017080349894967758 local 16010 offload 10290\n",
      "Episode: 263  Steps: 99 Reward 0.01688875539871028 local 16105 offload 10295\n",
      "Episode: 264  Steps: 99 Reward 0.01700950033450451 local 16202 offload 10298\n",
      "Episode: 265  Steps: 99 Reward 0.018144065918720836 local 16294 offload 10306\n",
      "Episode: 266  Steps: 99 Reward 0.017626109289005435 local 16392 offload 10308\n",
      "Episode: 267  Steps: 99 Reward 0.01775895005310397 local 16490 offload 10310\n",
      "Episode: 268  Steps: 99 Reward 0.01697921967680266 local 16583 offload 10317\n",
      "Episode: 269  Steps: 99 Reward 0.01774840772686304 local 16680 offload 10320\n",
      "Episode: 270  Steps: 99 Reward 0.01745282531563383 local 16777 offload 10323\n",
      "Episode: 271  Steps: 99 Reward 0.01704322969230511 local 16875 offload 10325\n",
      "Episode: 272  Steps: 99 Reward 0.016624791105328263 local 16971 offload 10329\n",
      "Episode: 273  Steps: 99 Reward 0.01721815380376328 local 17065 offload 10335\n",
      "Episode: 274  Steps: 99 Reward 0.01707275177865027 local 17161 offload 10339\n",
      "Episode: 275  Steps: 99 Reward 0.01956365503083907 local 17252 offload 10348\n",
      "Episode: 276  Steps: 99 Reward 0.018277624231911392 local 17348 offload 10352\n",
      "Episode: 277  Steps: 99 Reward 0.01832551904048158 local 17442 offload 10358\n",
      "Episode: 278  Steps: 99 Reward 0.016642656325379326 local 17536 offload 10364\n",
      "Episode: 279  Steps: 99 Reward 0.016347925983933406 local 17633 offload 10367\n",
      "Episode: 280  Steps: 99 Reward 0.01834216577117957 local 17724 offload 10376\n",
      "Episode: 281  Steps: 99 Reward 0.016366897783377642 local 17821 offload 10379\n",
      "Episode: 282  Steps: 99 Reward 0.017481112315594335 local 17913 offload 10387\n",
      "Episode: 283  Steps: 99 Reward 0.017829094388152566 local 18004 offload 10396\n",
      "Episode: 284  Steps: 99 Reward 0.017939393065536106 local 18098 offload 10402\n",
      "Episode: 285  Steps: 99 Reward 0.01713007574408854 local 18193 offload 10407\n",
      "Episode: 286  Steps: 99 Reward 0.016508971062393896 local 18288 offload 10412\n",
      "Episode: 287  Steps: 99 Reward 0.01635185219992553 local 18382 offload 10418\n",
      "Episode: 288  Steps: 99 Reward 0.016975034973110325 local 18480 offload 10420\n",
      "Episode: 289  Steps: 99 Reward 0.017445538984243166 local 18578 offload 10422\n",
      "Episode: 290  Steps: 99 Reward 0.016242377630085125 local 18676 offload 10424\n",
      "Episode: 291  Steps: 99 Reward 0.01770977977426996 local 18772 offload 10428\n",
      "Episode: 292  Steps: 99 Reward 0.01622792066250441 local 18870 offload 10430\n",
      "Episode: 293  Steps: 99 Reward 0.01601595599550216 local 18966 offload 10434\n",
      "Episode: 294  Steps: 99 Reward 0.0177824376603664 local 19060 offload 10440\n",
      "Episode: 295  Steps: 99 Reward 0.016380511045270475 local 19156 offload 10444\n",
      "Episode: 296  Steps: 99 Reward 0.017082661763835483 local 19248 offload 10452\n",
      "Episode: 297  Steps: 99 Reward 0.018495344096622714 local 19343 offload 10457\n",
      "Episode: 298  Steps: 99 Reward 0.016837107975894015 local 19437 offload 10463\n",
      "Episode: 299  Steps: 99 Reward 0.017920241194177445 local 19532 offload 10468\n",
      "Episode: 300  Steps: 99 Reward 0.017499607134945836 local 19623 offload 10477\n",
      "Episode: 301  Steps: 99 Reward 0.01697565746442458 local 19717 offload 10483\n",
      "Episode: 302  Steps: 99 Reward 0.016830519737237676 local 19806 offload 10494\n",
      "Episode: 303  Steps: 99 Reward 0.016399759209490588 local 19902 offload 10498\n",
      "Episode: 304  Steps: 99 Reward 0.017016164616879957 local 19997 offload 10503\n",
      "Episode: 305  Steps: 99 Reward 0.018369976979219084 local 20089 offload 10511\n",
      "Episode: 306  Steps: 99 Reward 0.01703587265809496 local 20186 offload 10514\n",
      "Episode: 307  Steps: 99 Reward 0.016758871753251536 local 20283 offload 10517\n",
      "Episode: 308  Steps: 99 Reward 0.01737917285640808 local 20379 offload 10521\n",
      "Episode: 309  Steps: 99 Reward 0.016763099851530537 local 20473 offload 10527\n",
      "Episode: 310  Steps: 99 Reward 0.017619578715231184 local 20566 offload 10534\n",
      "Episode: 311  Steps: 99 Reward 0.01818944527615504 local 20658 offload 10542\n",
      "Episode: 312  Steps: 99 Reward 0.018090944441414247 local 20751 offload 10549\n",
      "Episode: 313  Steps: 99 Reward 0.01775157856725246 local 20844 offload 10556\n",
      "Episode: 314  Steps: 99 Reward 0.017709319892998635 local 20936 offload 10564\n",
      "Episode: 315  Steps: 99 Reward 0.017622180192755032 local 21032 offload 10568\n",
      "Episode: 316  Steps: 99 Reward 0.01862622922946595 local 21124 offload 10576\n",
      "Episode: 317  Steps: 99 Reward 0.01731072406767383 local 21219 offload 10581\n",
      "Episode: 318  Steps: 99 Reward 0.01654392101273442 local 21314 offload 10586\n",
      "Episode: 319  Steps: 99 Reward 0.017431753495483388 local 21410 offload 10590\n",
      "Episode: 320  Steps: 99 Reward 0.01728272438149981 local 21508 offload 10592\n",
      "Episode: 321  Steps: 99 Reward 0.01818745476936571 local 21602 offload 10598\n",
      "Episode: 322  Steps: 99 Reward 0.017907439040431225 local 21695 offload 10605\n",
      "Episode: 323  Steps: 99 Reward 0.018014462436203763 local 21789 offload 10611\n",
      "Episode: 324  Steps: 99 Reward 0.018902501459041002 local 21876 offload 10624\n",
      "Episode: 325  Steps: 99 Reward 0.016989898924525043 local 21971 offload 10629\n",
      "Episode: 326  Steps: 99 Reward 0.01693294245727932 local 22068 offload 10632\n",
      "Episode: 327  Steps: 99 Reward 0.017983000478416677 local 22165 offload 10635\n",
      "Episode: 328  Steps: 99 Reward 0.017679319702755474 local 22259 offload 10641\n",
      "Episode: 329  Steps: 99 Reward 0.01747127089989488 local 22354 offload 10646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 330  Steps: 99 Reward 0.015786612083463124 local 22450 offload 10650\n",
      "Episode: 331  Steps: 99 Reward 0.018486965115590025 local 22543 offload 10657\n",
      "Episode: 332  Steps: 99 Reward 0.01651496110762643 local 22639 offload 10661\n",
      "Episode: 333  Steps: 99 Reward 0.0164070831603837 local 22735 offload 10665\n",
      "Episode: 334  Steps: 99 Reward 0.01658643560128214 local 22830 offload 10670\n",
      "Episode: 335  Steps: 99 Reward 0.017842642701747343 local 22923 offload 10677\n",
      "Episode: 336  Steps: 99 Reward 0.017031391663998564 local 23020 offload 10680\n",
      "Episode: 337  Steps: 99 Reward 0.017765172082902757 local 23116 offload 10684\n",
      "Episode: 338  Steps: 99 Reward 0.01882932090846909 local 23210 offload 10690\n",
      "Episode: 339  Steps: 99 Reward 0.01863561786645883 local 23305 offload 10695\n",
      "Episode: 340  Steps: 99 Reward 0.017432240741680403 local 23397 offload 10703\n",
      "Episode: 341  Steps: 99 Reward 0.01714859647727466 local 23489 offload 10711\n",
      "Episode: 342  Steps: 99 Reward 0.017398647638900887 local 23584 offload 10716\n",
      "Episode: 343  Steps: 99 Reward 0.016281489836329512 local 23678 offload 10722\n",
      "Episode: 344  Steps: 99 Reward 0.016583398286867005 local 23773 offload 10727\n",
      "Episode: 345  Steps: 99 Reward 0.01746382802282438 local 23866 offload 10734\n",
      "Episode: 346  Steps: 99 Reward 0.016207049474587194 local 23961 offload 10739\n",
      "Episode: 347  Steps: 99 Reward 0.01812730614530502 local 24050 offload 10750\n",
      "Episode: 348  Steps: 99 Reward 0.017663014208234032 local 24147 offload 10753\n",
      "Episode: 349  Steps: 99 Reward 0.0170572112920459 local 24245 offload 10755\n",
      "Episode: 350  Steps: 99 Reward 0.016937567827225216 local 24340 offload 10760\n",
      "Episode: 351  Steps: 99 Reward 0.017405090728681807 local 24436 offload 10764\n",
      "Episode: 352  Steps: 99 Reward 0.016981830843755638 local 24535 offload 10765\n",
      "Episode: 353  Steps: 99 Reward 0.017318231867066016 local 24633 offload 10767\n",
      "Episode: 354  Steps: 99 Reward 0.017010065162198856 local 24729 offload 10771\n",
      "Episode: 355  Steps: 99 Reward 0.01754311622098534 local 24825 offload 10775\n",
      "Episode: 356  Steps: 99 Reward 0.01734980581528319 local 24925 offload 10775\n",
      "Episode: 357  Steps: 99 Reward 0.01689566871809457 local 25022 offload 10778\n",
      "Episode: 358  Steps: 99 Reward 0.01724530916886309 local 25118 offload 10782\n",
      "Episode: 359  Steps: 99 Reward 0.016727919510641665 local 25212 offload 10788\n",
      "Episode: 360  Steps: 99 Reward 0.01712934629989381 local 25310 offload 10790\n",
      "Episode: 361  Steps: 99 Reward 0.017168076816825313 local 25407 offload 10793\n",
      "Episode: 362  Steps: 99 Reward 0.017908500287520772 local 25503 offload 10797\n",
      "Episode: 363  Steps: 99 Reward 0.017189061374287745 local 25598 offload 10802\n",
      "Episode: 364  Steps: 99 Reward 0.016807887588712902 local 25691 offload 10809\n",
      "Episode: 365  Steps: 99 Reward 0.017379862587787575 local 25786 offload 10814\n",
      "Episode: 366  Steps: 99 Reward 0.01674531482016533 local 25882 offload 10818\n",
      "Episode: 367  Steps: 99 Reward 0.017021843564624363 local 25976 offload 10824\n",
      "Episode: 368  Steps: 99 Reward 0.01815643818282439 local 26068 offload 10832\n",
      "Episode: 369  Steps: 99 Reward 0.01685825922680288 local 26162 offload 10838\n",
      "Episode: 370  Steps: 99 Reward 0.01877082117161045 local 26260 offload 10840\n",
      "Episode: 371  Steps: 99 Reward 0.0177688138520412 local 26359 offload 10841\n",
      "Episode: 372  Steps: 99 Reward 0.017422900270863156 local 26453 offload 10847\n",
      "Episode: 373  Steps: 99 Reward 0.016706699478892754 local 26542 offload 10858\n",
      "Episode: 374  Steps: 99 Reward 0.017693949798587162 local 26639 offload 10861\n",
      "Episode: 375  Steps: 99 Reward 0.016637903278035594 local 26735 offload 10865\n",
      "Episode: 376  Steps: 99 Reward 0.016523968423415518 local 26831 offload 10869\n",
      "Episode: 377  Steps: 99 Reward 0.017371342807931223 local 26927 offload 10873\n",
      "Episode: 378  Steps: 99 Reward 0.017923074878560933 local 27022 offload 10878\n",
      "Episode: 379  Steps: 99 Reward 0.01777232556561918 local 27116 offload 10884\n",
      "Episode: 380  Steps: 99 Reward 0.01814330677957552 local 27208 offload 10892\n",
      "Episode: 381  Steps: 99 Reward 0.017741042504327197 local 27304 offload 10896\n",
      "Episode: 382  Steps: 99 Reward 0.017940851233764432 local 27396 offload 10904\n",
      "Episode: 383  Steps: 99 Reward 0.017510706954363967 local 27488 offload 10912\n",
      "Episode: 384  Steps: 99 Reward 0.016329431770169864 local 27584 offload 10916\n",
      "Episode: 385  Steps: 99 Reward 0.016991925602504475 local 27678 offload 10922\n",
      "Episode: 386  Steps: 99 Reward 0.016951186110491057 local 27774 offload 10926\n",
      "Episode: 387  Steps: 99 Reward 0.017748632856405707 local 27870 offload 10930\n",
      "Episode: 388  Steps: 99 Reward 0.017722379907978356 local 27964 offload 10936\n",
      "Episode: 389  Steps: 99 Reward 0.016662558010048 local 28059 offload 10941\n",
      "Episode: 390  Steps: 99 Reward 0.018520095990174526 local 28153 offload 10947\n",
      "Episode: 391  Steps: 99 Reward 0.017459046835655398 local 28249 offload 10951\n",
      "Episode: 392  Steps: 99 Reward 0.016737548072584915 local 28347 offload 10953\n",
      "Episode: 393  Steps: 99 Reward 0.016921464944134244 local 28444 offload 10956\n",
      "Episode: 394  Steps: 99 Reward 0.017056984682361782 local 28537 offload 10963\n",
      "Episode: 395  Steps: 99 Reward 0.016425063110158965 local 28632 offload 10968\n",
      "Episode: 396  Steps: 99 Reward 0.01733287667066261 local 28725 offload 10975\n",
      "Episode: 397  Steps: 99 Reward 0.017493532555485677 local 28821 offload 10979\n",
      "Episode: 398  Steps: 99 Reward 0.01702363415331792 local 28918 offload 10982\n",
      "Episode: 399  Steps: 99 Reward 0.01778979289102981 local 29011 offload 10989\n",
      "Episode: 400  Steps: 99 Reward 0.0181084272939958 local 29109 offload 10991\n",
      "Episode: 401  Steps: 99 Reward 0.017936780890847166 local 29203 offload 10997\n",
      "Episode: 402  Steps: 99 Reward 0.017861955230794882 local 29299 offload 11001\n",
      "Episode: 403  Steps: 99 Reward 0.01727254447140121 local 29396 offload 11004\n",
      "Episode: 404  Steps: 99 Reward 0.017223791564666205 local 29492 offload 11008\n",
      "Episode: 405  Steps: 99 Reward 0.017767505249603553 local 29589 offload 11011\n",
      "Episode: 406  Steps: 99 Reward 0.01727887455213841 local 29687 offload 11013\n",
      "Episode: 407  Steps: 99 Reward 0.018206568679688347 local 29783 offload 11017\n",
      "Episode: 408  Steps: 99 Reward 0.017855247769156236 local 29880 offload 11020\n",
      "Episode: 409  Steps: 99 Reward 0.017337399180021088 local 29973 offload 11027\n",
      "Episode: 410  Steps: 99 Reward 0.017109562636743354 local 30069 offload 11031\n",
      "Episode: 411  Steps: 99 Reward 0.01623502984715637 local 30167 offload 11033\n",
      "Episode: 412  Steps: 99 Reward 0.01812113957569049 local 30259 offload 11041\n",
      "Episode: 413  Steps: 99 Reward 0.016583576678511635 local 30356 offload 11044\n",
      "Episode: 414  Steps: 99 Reward 0.017074769673222873 local 30453 offload 11047\n",
      "Episode: 415  Steps: 99 Reward 0.01866104034267325 local 30548 offload 11052\n",
      "Episode: 416  Steps: 99 Reward 0.017660396505769803 local 30643 offload 11057\n",
      "Episode: 417  Steps: 99 Reward 0.017344500871969464 local 30737 offload 11063\n",
      "Episode: 418  Steps: 99 Reward 0.01705829751409287 local 30832 offload 11068\n",
      "Episode: 419  Steps: 99 Reward 0.017330566467492824 local 30927 offload 11073\n",
      "Episode: 420  Steps: 99 Reward 0.017862937789577354 local 31024 offload 11076\n",
      "Episode: 421  Steps: 99 Reward 0.016581356394081637 local 31118 offload 11082\n",
      "Episode: 422  Steps: 99 Reward 0.01761923477939291 local 31210 offload 11090\n",
      "Episode: 423  Steps: 99 Reward 0.017755426835236215 local 31302 offload 11098\n",
      "Episode: 424  Steps: 99 Reward 0.0179964428520015 local 31396 offload 11104\n",
      "Episode: 425  Steps: 99 Reward 0.018253166501278194 local 31492 offload 11108\n",
      "Episode: 426  Steps: 99 Reward 0.017995672917903122 local 31585 offload 11115\n",
      "Episode: 427  Steps: 99 Reward 0.018625144426543824 local 31680 offload 11120\n",
      "Episode: 428  Steps: 99 Reward 0.018205858909839657 local 31773 offload 11127\n",
      "Episode: 429  Steps: 99 Reward 0.016817158974507508 local 31868 offload 11132\n",
      "Episode: 430  Steps: 99 Reward 0.017945938604539052 local 31966 offload 11134\n",
      "Episode: 431  Steps: 99 Reward 0.016845534913122847 local 32060 offload 11140\n",
      "Episode: 432  Steps: 99 Reward 0.017384435913292837 local 32151 offload 11149\n",
      "Episode: 433  Steps: 99 Reward 0.018016250166631866 local 32244 offload 11156\n",
      "Episode: 434  Steps: 99 Reward 0.017136846369117175 local 32337 offload 11163\n",
      "Episode: 435  Steps: 99 Reward 0.017360856005381942 local 32432 offload 11168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 436  Steps: 99 Reward 0.017438984642514237 local 32525 offload 11175\n",
      "Episode: 437  Steps: 99 Reward 0.01754242527195866 local 32622 offload 11178\n",
      "Episode: 438  Steps: 99 Reward 0.017611951012866646 local 32714 offload 11186\n",
      "Episode: 439  Steps: 99 Reward 0.017033652633528486 local 32807 offload 11193\n",
      "Episode: 440  Steps: 99 Reward 0.016756982466773616 local 32899 offload 11201\n",
      "Episode: 441  Steps: 99 Reward 0.016395659474473773 local 32994 offload 11206\n",
      "Episode: 442  Steps: 99 Reward 0.017361852885567818 local 33089 offload 11211\n",
      "Episode: 443  Steps: 99 Reward 0.017514852100517654 local 33184 offload 11216\n",
      "Episode: 444  Steps: 99 Reward 0.017707734632256146 local 33281 offload 11219\n",
      "Episode: 445  Steps: 99 Reward 0.016667986873411162 local 33375 offload 11225\n",
      "Episode: 446  Steps: 99 Reward 0.017051772826754474 local 33469 offload 11231\n",
      "Episode: 447  Steps: 99 Reward 0.018562549748374898 local 33561 offload 11239\n",
      "Episode: 448  Steps: 99 Reward 0.017960293042451225 local 33654 offload 11246\n",
      "Episode: 449  Steps: 99 Reward 0.0173214746961269 local 33749 offload 11251\n",
      "Episode: 450  Steps: 99 Reward 0.017326021185279136 local 33845 offload 11255\n",
      "Episode: 451  Steps: 99 Reward 0.01672012567730163 local 33942 offload 11258\n",
      "Episode: 452  Steps: 99 Reward 0.018876694659631918 local 34036 offload 11264\n",
      "Episode: 453  Steps: 99 Reward 0.017376035782329134 local 34131 offload 11269\n",
      "Episode: 454  Steps: 99 Reward 0.01778744524427108 local 34224 offload 11276\n",
      "Episode: 455  Steps: 99 Reward 0.017073743008517522 local 34318 offload 11282\n",
      "Episode: 456  Steps: 99 Reward 0.016716354712442722 local 34412 offload 11288\n",
      "Episode: 457  Steps: 99 Reward 0.016659733205458098 local 34506 offload 11294\n",
      "Episode: 458  Steps: 99 Reward 0.017528882256528096 local 34601 offload 11299\n",
      "Episode: 459  Steps: 99 Reward 0.017439317498228805 local 34696 offload 11304\n",
      "Episode: 460  Steps: 99 Reward 0.017888380035214858 local 34793 offload 11307\n",
      "Episode: 461  Steps: 99 Reward 0.018798911915221494 local 34884 offload 11316\n",
      "Episode: 462  Steps: 99 Reward 0.017937806746181298 local 34977 offload 11323\n",
      "Episode: 463  Steps: 99 Reward 0.017597868856790603 local 35073 offload 11327\n",
      "Episode: 464  Steps: 99 Reward 0.01732320506492555 local 35164 offload 11336\n",
      "Episode: 465  Steps: 99 Reward 0.017613903002782916 local 35263 offload 11337\n",
      "Episode: 466  Steps: 99 Reward 0.015609837506667527 local 35358 offload 11342\n",
      "Episode: 467  Steps: 99 Reward 0.017783217853879416 local 35451 offload 11349\n",
      "Episode: 468  Steps: 99 Reward 0.018174121807419417 local 35545 offload 11355\n",
      "Episode: 469  Steps: 99 Reward 0.0177810075530975 local 35638 offload 11362\n",
      "Episode: 470  Steps: 99 Reward 0.016603378847716975 local 35729 offload 11371\n",
      "Episode: 471  Steps: 99 Reward 0.017271493719960413 local 35825 offload 11375\n",
      "Episode: 472  Steps: 99 Reward 0.018042079308483273 local 35924 offload 11376\n",
      "Episode: 473  Steps: 99 Reward 0.017345286835055124 local 36017 offload 11383\n",
      "Episode: 474  Steps: 99 Reward 0.016958259787404645 local 36112 offload 11388\n",
      "Episode: 475  Steps: 99 Reward 0.01818965083647425 local 36207 offload 11393\n",
      "Episode: 476  Steps: 99 Reward 0.01609035194764512 local 36305 offload 11395\n",
      "Episode: 477  Steps: 99 Reward 0.01719502143168957 local 36401 offload 11399\n",
      "Episode: 478  Steps: 99 Reward 0.017026996440839378 local 36493 offload 11407\n",
      "Episode: 479  Steps: 99 Reward 0.017639171464734607 local 36588 offload 11412\n",
      "Episode: 480  Steps: 99 Reward 0.01773883225789397 local 36681 offload 11419\n",
      "Episode: 481  Steps: 99 Reward 0.01734715850268138 local 36779 offload 11421\n",
      "Episode: 482  Steps: 99 Reward 0.0167593661480563 local 36875 offload 11425\n",
      "Episode: 483  Steps: 99 Reward 0.016829722920228486 local 36968 offload 11432\n",
      "Episode: 484  Steps: 99 Reward 0.01705596273790165 local 37064 offload 11436\n",
      "Episode: 485  Steps: 99 Reward 0.016802316939287017 local 37158 offload 11442\n",
      "Episode: 486  Steps: 99 Reward 0.017229211203631637 local 37256 offload 11444\n",
      "Episode: 487  Steps: 99 Reward 0.018229098162660332 local 37343 offload 11457\n",
      "Episode: 488  Steps: 99 Reward 0.01790827490469254 local 37433 offload 11467\n",
      "Episode: 489  Steps: 99 Reward 0.01752834855451602 local 37529 offload 11471\n",
      "Episode: 490  Steps: 99 Reward 0.016603657649834555 local 37624 offload 11476\n",
      "Episode: 491  Steps: 99 Reward 0.016774437201329442 local 37722 offload 11478\n",
      "Episode: 492  Steps: 99 Reward 0.017025462941423063 local 37821 offload 11479\n",
      "Episode: 493  Steps: 99 Reward 0.01791407041287299 local 37914 offload 11486\n",
      "Episode: 494  Steps: 99 Reward 0.01674465645179191 local 38012 offload 11488\n",
      "Episode: 495  Steps: 99 Reward 0.017698628376139284 local 38108 offload 11492\n",
      "Episode: 496  Steps: 99 Reward 0.017515445378466044 local 38202 offload 11498\n",
      "Episode: 497  Steps: 99 Reward 0.017227038117992158 local 38299 offload 11501\n",
      "Episode: 498  Steps: 99 Reward 0.0173824430978 local 38397 offload 11503\n",
      "Episode: 499  Steps: 99 Reward 0.017612082479593383 local 38491 offload 11509\n",
      "Episode: 500  Steps: 99 Reward 0.015957261682938057 local 38587 offload 11513\n",
      "Episode: 501  Steps: 99 Reward 0.017114522025700435 local 38682 offload 11518\n",
      "Episode: 502  Steps: 99 Reward 0.01725832966583284 local 38776 offload 11524\n",
      "Episode: 503  Steps: 99 Reward 0.017230097442571025 local 38871 offload 11529\n",
      "Episode: 504  Steps: 99 Reward 0.017036897248437093 local 38969 offload 11531\n",
      "Episode: 505  Steps: 99 Reward 0.017194007869639292 local 39065 offload 11535\n",
      "Episode: 506  Steps: 99 Reward 0.016202108485676055 local 39157 offload 11543\n",
      "Episode: 507  Steps: 99 Reward 0.016703721833776834 local 39254 offload 11546\n",
      "Episode: 508  Steps: 99 Reward 0.017145091723382685 local 39352 offload 11548\n",
      "Episode: 509  Steps: 99 Reward 0.016492774511435965 local 39449 offload 11551\n",
      "Episode: 510  Steps: 99 Reward 0.017413254130771823 local 39542 offload 11558\n",
      "Episode: 511  Steps: 99 Reward 0.016293775308602158 local 39638 offload 11562\n",
      "Episode: 512  Steps: 99 Reward 0.018048177423802107 local 39733 offload 11567\n",
      "Episode: 513  Steps: 99 Reward 0.016044765255146892 local 39829 offload 11571\n",
      "Episode: 514  Steps: 99 Reward 0.017931386294050965 local 39924 offload 11576\n",
      "Episode: 515  Steps: 99 Reward 0.017382032479244348 local 40020 offload 11580\n",
      "Episode: 516  Steps: 99 Reward 0.017752584186895088 local 40113 offload 11587\n",
      "Episode: 517  Steps: 99 Reward 0.018076695712485283 local 40206 offload 11594\n",
      "Episode: 518  Steps: 99 Reward 0.017622340233300587 local 40298 offload 11602\n",
      "Episode: 519  Steps: 99 Reward 0.017710440635151255 local 40388 offload 11612\n",
      "Episode: 520  Steps: 99 Reward 0.01784885552192605 local 40481 offload 11619\n",
      "Episode: 521  Steps: 99 Reward 0.017980961854079647 local 40576 offload 11624\n",
      "Episode: 522  Steps: 99 Reward 0.017210001093592355 local 40673 offload 11627\n",
      "Episode: 523  Steps: 99 Reward 0.017577616977215528 local 40768 offload 11632\n",
      "Episode: 524  Steps: 99 Reward 0.017160755041282064 local 40865 offload 11635\n",
      "Episode: 525  Steps: 99 Reward 0.017503037920334717 local 40959 offload 11641\n",
      "Episode: 526  Steps: 99 Reward 0.017541707028009102 local 41054 offload 11646\n",
      "Episode: 527  Steps: 99 Reward 0.017494600391518737 local 41147 offload 11653\n",
      "Episode: 528  Steps: 99 Reward 0.016837320460967586 local 41241 offload 11659\n",
      "Episode: 529  Steps: 99 Reward 0.017639931287150426 local 41339 offload 11661\n",
      "Episode: 530  Steps: 99 Reward 0.017380208290175878 local 41435 offload 11665\n",
      "Episode: 531  Steps: 99 Reward 0.018850905776038054 local 41525 offload 11675\n",
      "Episode: 532  Steps: 99 Reward 0.0178002949973651 local 41622 offload 11678\n",
      "Episode: 533  Steps: 99 Reward 0.01737612749523205 local 41719 offload 11681\n",
      "Episode: 534  Steps: 99 Reward 0.016270473787710703 local 41818 offload 11682\n",
      "Episode: 535  Steps: 99 Reward 0.016552700455127595 local 41910 offload 11690\n",
      "Episode: 536  Steps: 99 Reward 0.016284540874707907 local 42006 offload 11694\n",
      "Episode: 537  Steps: 99 Reward 0.016475531710979315 local 42105 offload 11695\n",
      "Episode: 538  Steps: 99 Reward 0.016846332482666447 local 42196 offload 11704\n",
      "Episode: 539  Steps: 99 Reward 0.0183249073758976 local 42290 offload 11710\n",
      "Episode: 540  Steps: 99 Reward 0.01704371644971524 local 42382 offload 11718\n",
      "Episode: 541  Steps: 99 Reward 0.016995598806186233 local 42476 offload 11724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 542  Steps: 99 Reward 0.017315678151866264 local 42568 offload 11732\n",
      "Episode: 543  Steps: 99 Reward 0.017552819156417414 local 42662 offload 11738\n",
      "Episode: 544  Steps: 99 Reward 0.017387809866557195 local 42757 offload 11743\n",
      "Episode: 545  Steps: 99 Reward 0.01729238872478411 local 42852 offload 11748\n",
      "Episode: 546  Steps: 99 Reward 0.017016338021999 local 42946 offload 11754\n",
      "Episode: 547  Steps: 99 Reward 0.018461452703526192 local 43040 offload 11760\n",
      "Episode: 548  Steps: 99 Reward 0.017008045638669672 local 43135 offload 11765\n",
      "Episode: 549  Steps: 99 Reward 0.01832741374087381 local 43229 offload 11771\n",
      "Episode: 550  Steps: 99 Reward 0.017686784114214572 local 43322 offload 11778\n",
      "Episode: 551  Steps: 99 Reward 0.01744526376555301 local 43417 offload 11783\n",
      "Episode: 552  Steps: 99 Reward 0.01743850685218146 local 43510 offload 11790\n",
      "Episode: 553  Steps: 99 Reward 0.016439439101998278 local 43603 offload 11797\n",
      "Episode: 554  Steps: 99 Reward 0.017991650669860505 local 43696 offload 11804\n",
      "Episode: 555  Steps: 99 Reward 0.016358001882293055 local 43791 offload 11809\n",
      "Episode: 556  Steps: 99 Reward 0.016327260913538772 local 43885 offload 11815\n",
      "Episode: 557  Steps: 99 Reward 0.01759012663195944 local 43979 offload 11821\n",
      "Episode: 558  Steps: 99 Reward 0.01773641451330456 local 44073 offload 11827\n",
      "Episode: 559  Steps: 99 Reward 0.016824910243124543 local 44166 offload 11834\n",
      "Episode: 560  Steps: 99 Reward 0.017089995941924643 local 44265 offload 11835\n",
      "Episode: 561  Steps: 99 Reward 0.0171878535133051 local 44358 offload 11842\n",
      "Episode: 562  Steps: 99 Reward 0.018546050880958655 local 44450 offload 11850\n",
      "Episode: 563  Steps: 99 Reward 0.017699907141717087 local 44544 offload 11856\n",
      "Episode: 564  Steps: 99 Reward 0.016809469763196326 local 44641 offload 11859\n",
      "Episode: 565  Steps: 99 Reward 0.017845128351141246 local 44736 offload 11864\n",
      "Episode: 566  Steps: 99 Reward 0.01614438384433035 local 44833 offload 11867\n",
      "Episode: 567  Steps: 99 Reward 0.017642565643651097 local 44927 offload 11873\n",
      "Episode: 568  Steps: 99 Reward 0.018075939308687995 local 45023 offload 11877\n",
      "Episode: 569  Steps: 99 Reward 0.017077131417731956 local 45121 offload 11879\n",
      "Episode: 570  Steps: 99 Reward 0.01762201345399152 local 45212 offload 11888\n",
      "Episode: 571  Steps: 99 Reward 0.017497826823072014 local 45307 offload 11893\n",
      "Episode: 572  Steps: 99 Reward 0.01681949294262028 local 45402 offload 11898\n",
      "Episode: 573  Steps: 99 Reward 0.016265031469403737 local 45500 offload 11900\n",
      "Episode: 574  Steps: 99 Reward 0.018011278213541138 local 45598 offload 11902\n",
      "Episode: 575  Steps: 99 Reward 0.017856917310210132 local 45693 offload 11907\n",
      "Episode: 576  Steps: 99 Reward 0.01787352163472582 local 45785 offload 11915\n",
      "Episode: 577  Steps: 99 Reward 0.017671922928489233 local 45878 offload 11922\n",
      "Episode: 578  Steps: 99 Reward 0.017485829587188816 local 45976 offload 11924\n",
      "Episode: 579  Steps: 99 Reward 0.017825016370791574 local 46071 offload 11929\n",
      "Episode: 580  Steps: 99 Reward 0.017591648307473035 local 46163 offload 11937\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 190>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    242\u001b[0m     env\u001b[38;5;241m.\u001b[39mreward_list\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m RL\u001b[38;5;241m.\u001b[39mmemory_counter \u001b[38;5;241m>\u001b[39m MEMORY_CAPACITY:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# var = max([var * 0.9997, VAR_MIN])  # decay the action randomness\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m         \u001b[43mRL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# episode_list = np.append(episode_list, ep_reward)\u001b[39;00m\n\u001b[0;32m    249\u001b[0m episode_list\u001b[38;5;241m.\u001b[39mappend(env\u001b[38;5;241m.\u001b[39mshow_reward(T))\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    153\u001b[0m     sample_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory_counter, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m    155\u001b[0m batch_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory[sample_index, :]\n\u001b[1;32m--> 157\u001b[0m _, cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_memory\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_memory\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mr\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_memory\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms_\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_memory\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_features\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcost_his\u001b[38;5;241m.\u001b[39mappend(cost)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearn_step_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    971\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1191\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1194\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1368\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1371\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m   1377\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1380\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1361\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[0;32m   1359\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[0;32m   1360\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[1;32m-> 1361\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1454\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1453\u001b[0m                         run_metadata):\n\u001b[1;32m-> 1454\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from ENV import Environment\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "MAX_EPISODES = 200\n",
    "MEMORY_CAPACITY = 10000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "# Deep Q Network off-policy\n",
    "class DQN(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_actions,\n",
    "            n_features,\n",
    "            learning_rate=0.1,\n",
    "            reward_decay=0.001,\n",
    "            e_greedy=0.99,\n",
    "            replace_target_iter=200,\n",
    "            memory_size=MEMORY_CAPACITY,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            # e_greedy_increment=8.684615e-05,\n",
    "            # e_greedy_increment=None,\n",
    "            output_graph=False,\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon_max = e_greedy\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        # self.epsilon_increment = e_greedy_increment\n",
    "        # self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max\n",
    "        self.epsilon = 0.9\n",
    "        # self.epsilon = 0.9\n",
    "\n",
    "        # total learning step\n",
    "        self.learn_step_counter = 0\n",
    "\n",
    "        # initialize zero memory [s, a, r, s_]\n",
    "        # memory里存放当前和下一个state，动作和奖励\n",
    "        self.memory = np.zeros(\n",
    "            (MEMORY_CAPACITY, n_features * 2 + 2), dtype=np.float32)\n",
    "\n",
    "        # consist of [target_net, evaluate_net]\n",
    "        self._build_net()\n",
    "\n",
    "        t_params = tf.get_collection(\n",
    "            tf.GraphKeys.GLOBAL_VARIABLES, scope='target_net')\n",
    "        e_params = tf.get_collection(\n",
    "            tf.GraphKeys.GLOBAL_VARIABLES, scope='eval_net')\n",
    "\n",
    "        with tf.variable_scope('hard_replacement'):\n",
    "            self.target_replace_op = [\n",
    "                tf.assign(t, e) for t, e in zip(t_params, e_params)]\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.cost_his = []\n",
    "\n",
    "    def _build_net(self):\n",
    "        # ------------------ all inputs ------------------------\n",
    "        self.s = tf.placeholder(\n",
    "            tf.float32, [None, self.n_features], name='s')  # input State\n",
    "        self.s_ = tf.placeholder(\n",
    "            tf.float32, [None, self.n_features], name='s_')  # input Next State\n",
    "        self.r = tf.placeholder(tf.float32, [None, ], name='r')  # input Reward\n",
    "        self.a = tf.placeholder(tf.int32, [None, ], name='a')  # input Action\n",
    "\n",
    "        w_initializer, b_initializer = tf.random_normal_initializer(\n",
    "            0., 0.3), tf.constant_initializer(0.1)\n",
    "\n",
    "        # ------------------ build evaluate_net ------------------\n",
    "        with tf.variable_scope('eval_net'):\n",
    "            e1 = tf.layers.dense(self.s, 100, tf.nn.relu6, kernel_initializer=w_initializer,\n",
    "                                 bias_initializer=b_initializer, name='e1')\n",
    "            # e2 = tf.layers.dense(e1, 48, tf.nn.relu6, kernel_initializer=w_initializer,\n",
    "            #                      bias_initializer=b_initializer, name='e2')\n",
    "            e3 = tf.layers.dense(e1, 20, tf.nn.relu, kernel_initializer=w_initializer,\n",
    "                                 bias_initializer=b_initializer, name='e3')\n",
    "            self.q_eval = tf.layers.dense(e3, self.n_actions, tf.nn.softmax, kernel_initializer=w_initializer,\n",
    "                                          bias_initializer=b_initializer, name='q')\n",
    "\n",
    "        # ------------------ build target_net ------------------\n",
    "        with tf.variable_scope('target_net'):\n",
    "            t1 = tf.layers.dense(self.s_, 100, tf.nn.relu6, kernel_initializer=w_initializer,\n",
    "                                 bias_initializer=b_initializer, name='t1')\n",
    "            # t2 = tf.layers.dense(t1, 48, tf.nn.relu6, kernel_initializer=w_initializer,\n",
    "            #                      bias_initializer=b_initializer, name='t2')\n",
    "            t3 = tf.layers.dense(t1, 20, tf.nn.relu, kernel_initializer=w_initializer,\n",
    "                                 bias_initializer=b_initializer, name='t3')\n",
    "            self.q_next = tf.layers.dense(t3, self.n_actions, tf.nn.softmax, kernel_initializer=w_initializer,\n",
    "                                          bias_initializer=b_initializer, name='t4')\n",
    "\n",
    "        with tf.variable_scope('q_target'):\n",
    "            q_target = self.r + self.gamma * \\\n",
    "                tf.reduce_max(self.q_next, axis=1,\n",
    "                              name='Qmax_s_')  # shape=(None, )\n",
    "            self.q_target = tf.stop_gradient(q_target)\n",
    "        with tf.variable_scope('q_eval'):\n",
    "            a_indices = tf.stack(\n",
    "                [tf.range(tf.shape(self.a)[0], dtype=tf.int32), self.a], axis=1)\n",
    "            self.q_eval_wrt_a = tf.gather_nd(\n",
    "                params=self.q_eval, indices=a_indices)  # shape=(None, )\n",
    "        with tf.variable_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(\n",
    "                self.q_target, self.q_eval_wrt_a, name='TD_error'))\n",
    "        with tf.variable_scope('train'):\n",
    "            # self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)\n",
    "            self._train_op = tf.train.AdamOptimizer(\n",
    "                self.lr).minimize(self.loss)\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        if not hasattr(self, 'memory_counter'):\n",
    "            self.memory_counter = 0\n",
    "        transition = np.hstack((s, a, [r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # to have batch dimension when feed into tf placeholder\n",
    "        observation = observation[np.newaxis, :]\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # forward feed the observation and get q value for every actions\n",
    "            actions_value = self.sess.run(\n",
    "                self.q_eval, feed_dict={self.s: observation})\n",
    "            action = np.argmax(actions_value)\n",
    "        else:\n",
    "            action = np.random.randint(0, 2)\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        # check to replace target parameters\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self.sess.run(self.target_replace_op)\n",
    "            # print('\\ntarget_params_replaced\\n')\n",
    "\n",
    "        # sample batch memory from all memory\n",
    "        if self.memory_counter > self.memory_size:\n",
    "            sample_index = np.random.choice(\n",
    "                self.memory_size, size=self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(\n",
    "                self.memory_counter, size=self.batch_size)\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "\n",
    "        _, cost = self.sess.run(\n",
    "            [self._train_op, self.loss],\n",
    "            feed_dict={\n",
    "                self.s: batch_memory[:, :self.n_features],\n",
    "                self.a: batch_memory[:, self.n_features],\n",
    "                self.r: batch_memory[:, self.n_features + 1],\n",
    "                self.s_: batch_memory[:, -self.n_features:],\n",
    "            })\n",
    "\n",
    "        self.cost_his.append(cost)\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "\n",
    "env = Environment()\n",
    "action_space = ['0', '1']\n",
    "n_actions = len(action_space)\n",
    "n_features = 7\n",
    "RL = DQN(n_actions, n_features, output_graph=False)\n",
    "#QL = QLearningTable(actions=list(range(env.n_actions)))\n",
    "MAX_EPISODES = 3000\n",
    "T = 100\n",
    "# var = 1  # control exploration\n",
    "var = 0.1  # control exploration\n",
    "t1 = time.time()\n",
    "episode_list = []\n",
    "delay_list = []\n",
    "actions = []\n",
    "offload = 0\n",
    "local = 0\n",
    "penalty = 5\n",
    "delay = 0\n",
    "r = 0\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        time.sleep(0.1)\n",
    "    i = 0\n",
    "    for i in range(T):\n",
    "        obs = env.observed()\n",
    "        # print(\"obs\", obs[0], obs[1])\n",
    "        # Add exploration noise\n",
    "        #a0 = RL.choose_action_d(obs)\n",
    "        #print('DQN_a0', a0)\n",
    "        a = RL.choose_action(obs)\n",
    "        #print(\"DQN action\", a)\n",
    "\n",
    "        w_n = 0.2  # 0.1 or 0.99\n",
    "        f_n = 0.3  # 0.1 or 0.99\n",
    "        actions = [0.3, 0.4]\n",
    "\n",
    "        if a == 1:\n",
    "            # offload\n",
    "            R_n = w_n*obs[0]*env.Wmax * \\\n",
    "                math.log(1+(obs[5]/(w_n*obs[0]*env.Wmax*env.n0)), 2)\n",
    "            T_trans = obs[2] / R_n\n",
    "            T_MEC = obs[3] / (f_n*obs[1]*env.Fmax)\n",
    "            T_offload = round((T_trans + T_MEC), 5)\n",
    "            reward = env.step(actions, round(T_trans, 5), round(T_MEC, 5))\n",
    "\n",
    "            #reward = 10\n",
    "            s_ = obs.copy()\n",
    "            # print('S_', s_)\n",
    "            s_[0] -= (s_[0] * w_n)\n",
    "            s_[1] -= (s_[1] * f_n)\n",
    "            # print(obs, s_)\n",
    "            offload += 1\n",
    "\n",
    "        else:\n",
    "            # loacl\n",
    "            actions[0] = actions[1] = 0\n",
    "            T_local = obs[3] / obs[6]\n",
    "            E_local = env.kn * pow(obs[6], 2) * obs[3]\n",
    "            reward = env.alpha * T_local + env.beta * E_local\n",
    "\n",
    "            #reward = 10\n",
    "            # s_ = obs\n",
    "            s_ = obs.copy()\n",
    "            # s_[6] = 0\n",
    "            # print(obs, s_)\n",
    "            local += 1\n",
    "\n",
    "        #reward = reward + 0.5\n",
    "        RL.store_transition(obs, a, -reward, s_)\n",
    "\n",
    "        env.reward_list.append(reward)\n",
    "\n",
    "        if RL.memory_counter > MEMORY_CAPACITY:\n",
    "            # var = max([var * 0.9997, VAR_MIN])  # decay the action randomness\n",
    "            RL.learn()\n",
    "\n",
    "    # episode_list = np.append(episode_list, ep_reward)\n",
    "    episode_list.append(env.show_reward(T))\n",
    "    # print('reward', episode_list)\n",
    "    # print(\"actions\", actions)\n",
    "    print('Episode:', episode, ' Steps: %2d' % i, 'Reward',\n",
    "          episode_list[-1], \"local\", local, \"offload\", offload)\n",
    "    # print('OBS', obs)\n",
    "\n",
    "    # # Evaluate episode\n",
    "    # if (i + 1) % 50 == 0:\n",
    "    #     eval_policy(ddpg, env)\n",
    "\n",
    "print('Running time: ', time.time() - t1)\n",
    "plt.plot(episode_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.savefig(\"dqn.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
